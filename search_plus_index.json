{"./":{"url":"./","title":"Introduction","keywords":"","body":"Welcome to Memorygzs 友情链接：Memory影院 "},"MyNotes/GitBook相关.html":{"url":"MyNotes/GitBook相关.html","title":"GitBook相关","keywords":"","body":"GitBook相关 一.安装Gitbook实现本地预览 先下载 nodejs 并安装 Nodejs官网:https://nodejs.org/en 打开Node.js command prompt 或者git bash npm install gitbook-cli -g gitbook -V #如果有版本显示则安装gitbook成功 #创建存放gitbook文件夹(notes)并进入到相应的目录 gitbook init gitbook serve #用于本地搭建，如果要用于生产环境可不执行 gitbook build #其实在serve时就已经生成了对应的目录,默认为_book gitbook build . book #前面路径是以当前路径为构建目录,后者路径是生成路径，上面和这个命令选一条执行 在notes文件夹中创建笔记文件夹(MyNotes),在MyNotes创建相应的笔记文件 使用gitbook init后会自动生成两个文件README.md和SUMMARY.md README.md使用过git的都知道这个文件 SUMMARY.md就是自己要写文章章节目录 编辑SUMMARY.md [] 中括号里面写笔记名字 () 括号后面写笔记文件路径 比如: 在notes(当前文件夹)的MyNotes下创建aa.md: [aa](MyNoyes/aa.md) 在notes(当前文件夹)的MyNotes下创建bb文件夹且在文件夹里面添加aa.md: [aa](MyNoyes/bb/aa.md) # 目录，根据自己创建的文件夹而定,在SUMMARY.md文件中添加以下内容: * [前言](README.md) * [第一章](Chapter1/README.md) * [第1节：衣](Chapter1/衣.md) * [第2节：食](Chapter1/食.md) * [第3节：住](Chapter1/住.md) * [第4节：行](Chapter1/行.md) * [第二章](Chapter2/README.md) * [第三章](Chapter3/README.md) * [第四章](Chapter4/README.md) 配置目录折叠 npm install -g gitbook-plugin-splitter gitbook install 在主目录添加book.json(需要手动创建) { \"plugins\":[ \"expandable-chapters\" ] } 当我们编写完笔记之后，则重新生成对应的book文件夹 gitbook build . book 若要本地预览 gitbook server 二.展示到Github的page中 在Github中创建名为 用户名.github.io 的仓库 在 用户名.github.io 的项目里点击 settings ，往下翻找到 GitHub Pages 根据具体情况选择分支，一般选择master分支 将编译好的文件 PUSH 到远端仓库(进入到_book目录或者book目录) git add . git commit -m \"Inital commit\" git remote add origin https://github.com/xxxx/xxx.git git push -u orgin master -f 三.更新 将原来的_book目录下的.git 隐藏文件复制到根目录（没有则忽略） 在笔记文件夹中添加相关的笔记之后重新执行gitbook命名生成对应的_book目录 #不是在执行gitbook生成的_book目录下执行命令，而是最开始的笔记文件夹,有README.md文件的目录 gitbook build . book #此时会生成book文件夹，这个文件夹根据自己情况设置，然后我们可以重新执行git命令上传到github 进入到notes文件夹 重新创建.git文件夹 git init git add . git commit -m \"notes\" git remote add notes https://github.com/用户名/用户名.github.io.git #提交时,可以先使用git pull再git push git pull notes git push -u notes master #或则不执行上面两句，直接执行 git push -u notes master -f #强制覆盖原来的github中的所有文件 或者将刚刚复制的.git隐藏文件复制到此目录下 #上面命令作废,执行下列命令 git add . git commit -m \"notes\" git pull origin git push 四.常用插件 https://www.jianshu.com/p/427b8bb066e6 https://www.cnblogs.com/mingyue5826/p/10307051.html#autoid-2-3-0 "},"MyNotes/前端/Echarts相关.html":{"url":"MyNotes/前端/Echarts相关.html","title":"Echarts相关","keywords":"","body":"Echarts相关 生成Echarts对象绑定标签： ec = echarts.init(document.getElementById('hotwork')) 设置echarts属性： ec.setOption(workOption) workOption为图形的属性 折线图(line) workOption= { title:{ //标题 text:\"职位分析\", //主标题 subtext:\"--10大热门职位分析\", //副标题 x:'45%'， //标题位置，可以设置center，right等或者百分比 textStyle:{ //标题样式 color:#000000 } }, xAxis:{ //x轴的属性 type:'category', //x轴坐标类型，category类目轴，value数值轴，time时间轴 data:[], //x轴数据，列表 name:\"时间\", //x轴名字（标签） axisLabel:{ //设置x轴数据的一些属性 interval:0, //设置间隔,0表示强制显示所有的标签，1表示隔一个标签显示一个标签，2表示 隔两个标签显示一个标签，以此类推 rotate:'25' //x轴数据的倾斜度，逆时针旋转，-25是顺时针旋转 } }, yAxis:{ //属性基本上和xAxis一样 name:\"摄氏度°C\", type:'value' }, legend:{ //设置图列 data:['折线图'], //这里设置series中的name，注意和数据项轴（饼图等）不一样 left:'right' //设置图例的位置 }, series:[ //设置图形的属性,这里是一个列表包起来，因为一个图中可以画很多个图形 { name:'折线图', type:'line', //设置图形类型为折线图 data:[], //data是x轴数据对应的y轴数据 label:{ //显示数据并且设置显示的位置 show:true, position:top //在顶部显示数据 } } ], tooltip:{ //设置鼠标放到某个点的时候，会出现相应的x,y数据 show:true } } 柱状图&堆叠柱状图 barOption={ title:{ text:'大数据相关职位招聘数量', subtext:'--职位招聘对比'， x='center' }, xAxis:{ type:'category', name:'职位名称', data:['大数据开发','爬虫工程师','大数据运维'，'可视化工程师'], axisLabel:{ interval:0, rotate:'-25' } }, yAxis:[ //如果有多个y轴，则添加两个，以列表包起来，默认显示是左右两边有两条y轴 { name:'招聘数量', type:'value' }, { name:'测试', interval:5, //每一个数据相隔多少 min:1, //设置最小值 max:50 //设置最大值 } ], tooltip:{ show:true, trigger:'axis' }, legend:{ show:true, data:['长沙','武汉'] }, series:[ { name:'长沙' type:'bar', stack:'城市', //stack可以当作是一个组，如果是一个组的话，每个组中的柱状会上下堆叠 data:[100,200,300,400], label:{ normal:{ show:true, //显示数据文本在图上 position:'insideRigt' //设置显示的位置，inside是里面 } }， itemStyle:{ normal:{ color:'blue' //设置每个柱形的颜色 } } }, { name:'武汉', stack:'城市', type:'bar', data:[500,600,700,800], } ] } 南丁格尔图 bigdataOption={ title:{ text:\"所有城市招聘数据的平均工资vs“大数据”相关职位所有城市招聘数据的平均工资\", subtext:'南丁格尔玫瑰图', x:'center', }, tooltip:{ show:true, trigger:'item'， //触发的类型，item是数据项的图形触发，axis坐标轴触发 formatter:{b}:{c}:{d}%, //设置显示的格式，{b}数据名称，{c}数值，{d}百分比的值 }, legend:{ //设置图列 data:['长沙','武汉','成都']， //这里设置series中data数据列表中每一个字典的name值 left:'right' }, color: //设置图形中有多少颜色 [ '#C1232B','#B5C334','#FCCE10','#E87C25','#27727B', '#668ffe','#00ca54','#00dbfa','#f3006a','#60C0DD','#d714b7', '#84433c','#f490f3','#000000','#26C0C0' ], series:[ { type:'pie', //这里设置类型为pie饼图，南丁格尔图属于饼图的一种 radius:[20,110], //设置图形的大小，20是内圆半径，110是外圆半径 center:['25%','50%'], //设置图形的位置，25%是根据当前series宽度来判断，而50%根据高度来判断，相当于x和y的位置 roseType:'radius', //设置南丁格尔图，其中，radius类型是展现以百分比区分的扇区圆心角，且通过半径来显示，而area类型是指所有扇区的圆心角相同，根据半径来区分，只展现半径 data:[{name:'长沙',value:2000},{name:'武汉',value:3000}] }， { type:'pie', radis:[20,110], center:['75%','50%'], roseType:'area', data:[{name:'北京',value:5000},{name:'上海',value:8000}] } ] } 雷达图 radarOption={ title:{ text:'维度', subtext:'雷达图' } radar:{ name:{ textStyle:{ //设置维度名称的样式 color:'#fff', //#fff白色 backgroundColor:'#999', //#999灰色 borderRadius:3, //圆角的半径 padding:[5,5,1,5] //内边距[5,5,1,5]分别对应[上，右，下，左] } }, indicator:[ //有多少维度，max可选，官方建议选上 {name:'平均工资',max:20000}, {name:'招聘数量',max:10000}, {name:'产品',max:30000} ] }, series:[ { type:'radar', data:[ //列表里面包含字典，字典中value对应着每个维度的值 {name:\"长沙\",value:[15000,8000,20]}, {name:\"武汉\",value:[10000,9000,30]} ] } ] } 散点热力图 scantterOption={ title:{ text:'各城市大数据相关职位招聘数量', subtext:'--散点热力图' }, tooltip:{ show:true, trigger:'item' }, legend:{ show:true, data:['长沙','武汉'], top:'bottom'， left:'right' }, visualMap:{ min:0, //设置数据的最大和最小值 max:300, splitNumber:5, //把数据范围分成5个段 inRange:{ color: ['#50a3ba', '#eac736', '#d94e5d'] //设置散点的颜色 } }， geo:{ map:\"china\", label:{ emphasis:{ //设置地图中部的标签和多边形的高亮显示 show:true } } }， series:[ { type:'scatter', coordinateSystem:'geo', //设置成geo（地图）类型，地理坐标轴，cartesian2d二维的直角坐标系，polar极坐标系 data:[ {name:'长沙',value:20} {name:'武汉'3131} ] } ] } 词云图 //词云图 list=[['zookeeper','10000'],['hadoop','300000'],['hive','1656'],['spark','20000'],['sqoop','10000'],['hbase','25000'],['flume','23000'],['Kafka','28000']] var wordCloud = new Js2WordCloud(document.getElementById('wordCloud')); wordCloudOption = { tooltip: { show: true }, list: list, color: '#15a4fa', //字体颜色 backgroundColor: '#fff', shape: 'circle', ellipticity: 1 } wordCloud.setOption(wordCloudOption); "},"MyNotes/Linux相关/Centos8安装Docker.html":{"url":"MyNotes/Linux相关/Centos8安装Docker.html","title":"Centos8安装Docker","keywords":"","body":"Centos8安装docker 1、Docker 要求 CentOS 系统的内核版本高于 3.10 ，查看本页面的前提条件来验证你的CentOS 版本是否支持 Docker 。 通过 uname -r 命令查看你当前的内核版本 [root@epg-0001 ~]# uname -r 4.18.0-80.el8.x86_64 2、使用 root 权限登录 Centos。确保 yum 包更新到最新。 [root@epg-0001 ~]# yum update Repository AppStream is listed more than once in the configuration Repository BaseOS is listed more than once in the configuration Repository extras is listed more than once in the configuration Repository PowerTools is listed more than once in the configuration Repository centosplus is listed more than once in the configuration Last metadata expiration check: 0:09:18 ago on Fri 25 Dec 2020 09:04:58 PM CST. Dependencies resolved. Nothing to do. Complete! [root@epg-0001 ~]# 3、卸载旧版本（如果安装过旧版本的话） [root@epg-0001 ~]# yum remove docker docker-common docker-selinux docker-engine 4、安装需要的软件包， yum-util 提供yum-config-manager功能，另外两个是devicemapper驱动依赖的 [root@epg-0001 ~]# yum install -y yum-utils device-mapper-persistent-data lvm2 5、设置yum源 ⚠️：国外镜像一般很难访问，建议配置阿里云镜像。 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo [root@epg-0001 ~]# yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo Repository AppStream is listed more than once in the configuration Repository BaseOS is listed more than once in the configuration Repository extras is listed more than once in the configuration Repository PowerTools is listed more than once in the configuration Repository centosplus is listed more than once in the configuration Adding repo from: https://download.docker.com/linux/centos/docker-ce.repo [root@epg-0001 ~]# 6、可以查看所有仓库中所有docker版本，并选择特定版本安装 [root@epg-0001 ~]# yum list docker-ce --showduplicates | sort -r Repository AppStream is listed more than once in the configuration Repository BaseOS is listed more than once in the configuration Repository extras is listed more than once in the configuration Repository PowerTools is listed more than once in the configuration Repository centosplus is listed more than once in the configuration Last metadata expiration check: 0:00:02 ago on Fri 25 Dec 2020 09:20:58 PM CST. docker-ce.x86_64 3:20.10.1-3.el8 docker-ce-stable docker-ce.x86_64 3:20.10.0-3.el8 docker-ce-stable docker-ce.x86_64 3:19.03.14-3.el8 docker-ce-stable docker-ce.x86_64 3:19.03.13-3.el8 docker-ce-stable Docker CE Stable - x86_64 887 B/s | 7.1 kB 00:08 Available Packages [root@epg-0001 ~]# 7、安装docker [root@epg-0001 ~]# yum install docker-ce Repository AppStream is listed more than once in the configuration Repository BaseOS is listed more than once in the configuration Repository extras is listed more than once in the configuration Repository PowerTools is listed more than once in the configuration Repository centosplus is listed more than once in the configuration Docker CE Stable - x86_64 107 kB/s | 28 kB 00:00 Error: Problem: package docker-ce-3:20.10.1-3.el7.x86_64 requires containerd.io >= 1.4.1, but none of the providers can be installed - cannot install the best candidate for the job - package containerd.io-1.4.3-3.1.el7.x86_64 is filtered out by modular filtering (try to add '--skip-broken' to skip uninstallable packages or '--nobest' to use not only best candidate packages) [root@epg-0001 ~]# ⚠️：安装 docker-ce 报错： Error: Problem: package docker-ce-3:20.10.1-3.el7.x86_64 requires containerd.io >= 1.4.1, but none of the providers can be installed 于是安装containerd.io ： yum install https://download.docker.com/linux/centos/8/x86_64/stable/Packages/containerd.io-1.4.3-3.1.el8.x86_64.rpm ⚠️：如果出现 podman 冲突的问题，大致错误如下： [root@epg-0003 ~]# yum install https://download.docker.com/linux/centos/8/x86_64/stable/Packages/containerd.io-1.4.3-3.1.el8.x86_64.rpm Repository AppStream is listed more than once in the configuration Repository BaseOS is listed more than once in the configuration Repository extras is listed more than once in the configuration Repository PowerTools is listed more than once in the configuration Repository centosplus is listed more than once in the configuration Last metadata expiration check: 0:00:25 ago on Sun 27 Dec 2020 01:15:16 AM CST. containerd.io-1.4.3-3.1.el8.x86_64.rpm 1.1 MB/s | 33 MB 00:30 Error: Problem: problem with installed package podman-1.6.4-10.module_el8.2.0+305+5e198a41.x86_64 - package podman-1.6.4-10.module_el8.2.0+305+5e198a41.x86_64 requires runc >= 1.0.0-57, but none of the providers can be installed - package containerd.io-1.4.3-3.1.el8.x86_64 conflicts with runc provided by runc-1.0.0-65.rc10.module_el8.2.0+305+5e198a41.x86_64 - package containerd.io-1.4.3-3.1.el8.x86_64 obsoletes runc provided by runc-1.0.0-65.rc10.module_el8.2.0+305+5e198a41.x86_64 - conflicting requests - package runc-1.0.0-64.rc10.module_el8.2.0+304+65a3c2ac.x86_64 is filtered out by modular filtering - package containerd.io-1.2.10-3.2.el7.x86_64 is filtered out by modular filtering - package containerd.io-1.2.13-3.1.el7.x86_64 is filtered out by modular filtering - package containerd.io-1.2.13-3.2.el7.x86_64 is filtered out by modular filtering - package containerd.io-1.2.2-3.3.el7.x86_64 is filtered out by modular filtering - package containerd.io-1.2.2-3.el7.x86_64 is filtered out by modular filtering - package containerd.io-1.2.4-3.1.el7.x86_64 is filtered out by modular filtering - package containerd.io-1.2.5-3.1.el7.x86_64 is filtered out by modular filtering - package containerd.io-1.2.6-3.3.el7.x86_64 is filtered out by modular filtering - package containerd.io-1.3.7-3.1.el7.x86_64 is filtered out by modular filtering - package containerd.io-1.3.9-3.1.el7.x86_64 is filtered out by modular filtering - package containerd.io-1.4.3-3.1.el7.x86_64 is filtered out by modular filtering (try to add '--allowerasing' to command line to replace conflicting packages or '--skip-broken' to skip uninstallable packages or '--nobest' to use not only best candidate packages) 则可以执行如下命令解决 podmon 冲突： [root@epg-0003 ~]# yum erase podman buildah 然后重新安装 docker： yum install docker-ce #由于repo中默认只开启stable仓库，故这里安装的是最新稳定版 yum install # 例如： yum install docker-ce-3:20.10.1-3.el8 8、启动并加入开机启动 systemctl start docker systemctl enable docker 9、验证安装是否成功（有client和service两部分表示docker安装启动都成功了） [root@epg-0001 ~]# docker version Client: Docker Engine - Community Version: 20.10.1 API version: 1.41 Go version: go1.13.15 Git commit: 831ebea Built: Tue Dec 15 04:37:17 2020 OS/Arch: linux/amd64 Context: default Experimental: true Server: Docker Engine - Community Engine: Version: 20.10.1 API version: 1.41 (minimum version 1.12) Go version: go1.13.15 Git commit: f001486 Built: Tue Dec 15 04:35:42 2020 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.4.3 GitCommit: 269548fa27e0089a8b8278fc4fc781d7f65a939b runc: Version: 1.0.0-rc92 GitCommit: ff819c7e9184c13b7c2607fe6c30ae19403a7aff docker-init: Version: 0.19.0 GitCommit: de40ad0 [root@epg-0001 ~]# 确保网络模块开机自动加载 [root@ecs-2aae ~]# lsmod |grep overlay overlay 126976 10 [root@ecs-2aae ~]# lsmod | grep br_netfilter br_netfilter 24576 0 bridge 188416 1 br_netfilter [root@ecs-2aae ~]# #################################################### # 如上面命令无返回值输出或提示文件不存在，需执行以下命令 cat > /etc/modules-load.d/docker.conf 使桥接流量对 iptables 可见 cat > /etc/sysctl.d/k8s.conf 配置 docker mkdir /etc/docker #修改cgroup驱动为systemd[k8s官方推荐]、限制容器日志量、修改存储类型，最后的docker家目录可修改 cat > /etc/docker/daemon.json 验证 docker 是否正常 #查看docker信息，判断是否与配置一致 [root@ecs-2aae ~]# docker info Client: Context: default Debug Mode: false Plugins: app: Docker App (Docker Inc., v0.9.1-beta3) buildx: Build with BuildKit (Docker Inc., v0.5.0-docker) Server: Containers: 10 Running: 10 Paused: 0 Stopped: 0 Images: 9 Server Version: 20.10.1 Storage Driver: overlay2 Backing Filesystem: extfs Supports d_type: true Native Overlay Diff: true Logging Driver: json-file Cgroup Driver: cgroupfs Cgroup Version: 1 Plugins: Volume: local Network: bridge host ipvlan macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog Swarm: inactive Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc Default Runtime: runc Init Binary: docker-init containerd version: 269548fa27e0089a8b8278fc4fc781d7f65a939b runc version: ff819c7e9184c13b7c2607fe6c30ae19403a7aff init version: de40ad0 Security Options: seccomp Profile: default Kernel Version: 4.18.0-80.el8.x86_64 Operating System: CentOS Linux 8 (Core) OSType: linux Architecture: x86_64 CPUs: 4 Total Memory: 7.6GiB Name: ecs-2aae ID: TUMK:ZAXS:B73T:5BN5:G6AI:ON26:VQG6:NZOZ:RBE6:BVTX:LGEM:6PJO Docker Root Dir: /var/lib/docker Debug Mode: false Registry: https://index.docker.io/v1/ Labels: Experimental: false Insecure Registries: 127.0.0.0/8 Registry Mirrors: https://registry.docker-cn.com/ https://gj9l74ds.mirror.aliyuncs.com/ http://hub-mirror.c.163.com/ Live Restore Enabled: false WARNING: API is accessible on http://192.168.1.61:2375 without encryption. Access to the remote API is equivalent to root access on the host. Refer to the 'Docker daemon attack surface' section in the documentation for more information: https://docs.docker.com/engine/security/security/#docker-daemon-attack-surface WARNING: No blkio weight support WARNING: No blkio weight_device support [root@ecs-2aae ~]# 修改docker 默认存储路径 【docker 安装后的默认存储路径是 /var/lib/docker （查看上面的 Docker Root Dir 属性），这里存储着镜像、容器等相关文件。】 1、创建docker新的默认目录 mkdir -p /home/docker 2、修改docker的systemd的 docker.service的配置文件 不知道 配置文件在哪里可以使用systemd 命令显示一下. systemctl disable docker systemctl enable docker # 显示结果 Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service. 3、修改 docker.service 文件 vim /usr/lib/systemd/system/docker.service # 在里面的EXECStart的后面增加后如下: ExecStart=/usr/bin/dockerd --graph /home/docker 4、然后重启 docker systecmtl daemon-reload systemctl restart docker 添加用户到docker组 非root用户，无需sudo即可使用docker命令 #添加用户到docker组 usermod -aG docker #当前会话立即更新docker组 newgrp docker "},"MyNotes/Java相关笔记/SSM整合.html":{"url":"MyNotes/Java相关笔记/SSM整合.html","title":"SSM整合","keywords":"","body":"SSM整合 环境搭建 新建一个webapp的Maven工程 在pom.xml添加以下坐标 5.2.3.RELEASE 1.7.30 1.2.17 8.0.19 3.5.4 org.aspectj aspectjweaver 1.9.5 org.springframework spring-aop ${spring.version} org.springframework spring-context ${spring.version} org.springframework spring-web ${spring.version} org.springframework spring-webmvc ${spring.version} org.springframework spring-test ${spring.version} org.springframework spring-tx ${spring.version} org.springframework spring-jdbc ${spring.version} junit junit 4.13 mysql mysql-connector-java ${mysql.version} javax.servlet servlet-api 2.5 provided javax.servlet.jsp jsp-api 2.2 provided jstl jstl 1.2 log4j log4j ${log4j.version} org.slf4j slf4j-api ${slf4j.version} org.slf4j slf4j-log4j12 ${slf4j.version} org.mybatis mybatis ${mybatis.version} org.mybatis mybatis-spring 2.0.3 com.mchange c3p0 0.9.5.5 jar compile 创建dao包及接口 创建entity包及实体类 创建service包及接口和实现类 创建Controller包及类 配置Spring 在resource下创建Spring配置文件（applicationContext.xml） 添加以下内容： Spring整合SpringMVC 配置WEB-INF下的web.xml文件 Archetype Created Web Application DispatcherServlet org.springframework.web.servlet.DispatcherServlet contextConfigLocation classpath:springmvc.xml 1 DispatcherServlet / characterEncodingFilter org.springframework.web.filter.CharacterEncodingFilter encoding UTF-8 characterEncodingFilter /* 在resource下创建SpringMVC配置文件（springmvc.xml） 在web.xml加载Spring配置文件 org.springframework.web.context.ContextLoaderListener contextConfigLocation classpath:applicationContext.xml 测试 在Controller能用Spring的注解表示整合成功 Spring整合Mybatis 在Spring配置文件（applicationContext.xml）中添加以下内容： Mybatis配置文件可以不创建，把配置都写到spring中了 如果要创建，在sqlSessionFactory工厂里面添加下面内容即可指定配置文件： 创建了mybatis配置文件，一般是在里面配置别名: 配置别名,package是扫描pojo包下的所有实体类， 别名就是实体类的类名（首字母小写） --> 连接池的介绍 在idea中，如果用Maven方式创建，一般xml配置文件都放在resource中，并且idea只会编译resource中的xml，如果放在java/*下，则会忽略，不会编译 添加一个mapperLocations属性，指定加载xml文件的路径。 classpath：表示在classes目录中查找； *：通配符表示所有文件 **：表示所有目录下 测试 能在Service实现类中利用注解直接引用dao接口并且调用方法成功 Spring整合Mybatis配置事务管理 在spring配置文件中添加以下内容： "},"MyNotes/Java相关笔记/Spring相关.html":{"url":"MyNotes/Java相关笔记/Spring相关.html","title":"Spring相关","keywords":"","body":"Spring相关 Spring的优势 1 方便解耦，简化开发 通过spring提供的ioc容器，可以将对象间的依赖关系交给spring管理,避免硬编码造成的程序过渡耦合 2 AOP编程的支持 通过spring的aop功能可以实现面向切面编程 3 声明式的事务支持 4 方便集成其他第三方框架 IOC的概念和作用 IOC指的是控制反转,指的就是以前我们获取一个对象时采用的是自己创建一个的方式，这是一个主动的过程；而控制反转后，当我们需要对象时就跟工厂要，而工厂来帮我们创建或者查找对象，这是一个被动的过程。 这种被动接收对象的方式就是控制反转。 它的作用是削减计算机程序的耦合(解除代码中的依赖关系) （只能降低，并不能完全解除依赖关系） Spring的环境搭建（IDEA） 普通方式 注意：如果没有Spring选项，请进入IDEA的Plugins进行开启 Maven方式 在pom.xml中写入以下内容： org.springframework spring-context 5.2.3.RELEASE 或者自己添加Spring必要的环境包 按住快捷键：Ctrl+Alt+Shift+S 打开项目设置界面 在resources文件夹中新建applicationConfig.xml文件，添加以下内容： 基于Xml方式的IOC 创建service、dao、entity（实体类）、service中的impl（实现接口）包 在entity包中创建User实体类 在service包中创建AccountService接口 在service中的impl包中创建AccountService实现类 配置applicationConfig.xml文件，添加以下内容： bean相当于一个变量，把所有的类交给spring管理 id为名字，class为类的全限类型 新建Main测试类 public class Main { public static void main(String[] args) { //1.获取核心容器对象 ClassPathXmlApplicationContext ac = new ClassPathXmlApplicationContext(\"applicationConfig.xml\"); //2.根据id获取Bean对象 AccountService accountService = (AccountService) ac.getBean(\"accountService\"); //3.调用accountService中的acc方法 accountService.acc(); } } ApplicationContext和BeanFactory两个核心容器接口的区别 ApplicationContext（单例对象用，实际开发基本上都是用此接口） 它在构建核心容器时，创建对象采取的策略是采用立即加载的方式。也就是说，只要一读取配置文件马上就创建配置文件中配置的对象。 BeanFactory（多例对象用） 它在构建核心容器时，创建对象采取的策略是采用延迟加载的方式。也就是说，什么时候根据id获取对象了，什么时候才真正的创建对象。 Bean对象相关 创建bean的三种方式 bean对象的作用范围 作用域 描述 singleton （单例的 默认值）在spring IoC容器仅存在一个Bean实例，Bean以单例方式存在，bean作用域范围的默认值。（常用） prototype （多例的）每次从容器中调用Bean时，都返回一个新的实例，即每次调用getBean()时，相当于执行newXxxBean()。（常用） request 每次HTTP请求都会创建一个新的Bean，该作用域仅适用于web的Spring WebApplicationContext环境。（web应用的请求范围） session 同一个HTTP Session共享一个Bean，不同Session使用不同的Bean。该作用域仅适用于web的Spring WebApplicationContext环境。（web应用的会话范围） global-session 该作用域将 bean 的定义限制为全局 HTTP 会话。只在 web-aware Spring ApplicationContext 的上下文中有效。（集群环境的会话范围，全局会话返回，当不是集群环境时，它就是session） bean对象的生命周期 初始化 存活 销毁 总结 单例对象 当容器创建时对象初始化 只要容器还在，对象一直存活 容器销毁，对象销毁 单例对象的生命周期与容器相同 多例对象 当我们使用对象时spring容器为我们创建 对象只要在使用的过程中就一直存活 当对象长时间不用，且没有别的对象引用时，由java的垃圾回收机器回收 多例对象的生命周期不和容器相同 Spring的依赖注入 依赖关系的管理：以后都交给spring来维护 在当前类需要用到其他类的对象，由spring为我们提供，我们只需要在配置文件中说明 依赖关系的维护就称之为依赖注入 依赖注入能注入的数据有三类：基本类型和String、其他bean类型（在配置文件中或者注解配置过的bean） 注入的方式有三种：使用构造函数提供、使用set方法方法提供、使用注解提供 构造函数注入 constructor-arg标签属性： type：用于指定要注入的数据的数据类型，该数据类型页视构造函数中某个或某些参数的类型 index:用于指定要注入的数据给构造函数中指定索引位置的参数赋值。索引的位置是从0开始 name：用于指定给构造函数中指定名称的参数赋值 (常用的) value：用于指定的值，类型为基本类型和String类型的数据 ref：用于指定其他的bean类型数据，在spring IOC容器中出现过的bean对象 set方法注入 property标签属性： name：用于指定注入时所调用的set方法名称 value：用于指定的值，类型为基本类型和String类型的数据 ref：用于指定其他的bean类型数据，在spring IOC容器中出现过的bean对象 注解注入 基于注解方式的IOC 使用注解的前提:必须要在配置文件中配置扫描的包 且需要aop jar包 配置applicationConfig.xml文件，添加以下内容： @Component 使用@Component注解进行设置bean对象，如果后面没有填值，则默认是类名且首字母小写，后面如果有value值，则value值是bean对象的名字 @Component(value = \"accountService\") public class AccountServiceImpl implements AccountService { public void acc() { System.out.println(\"此方法是AccountService接口中的方法\"); } } @Controller（表现层）、@Service（业务层）、@Repository（持久层）对应三层架构 在对应的类中添加对应的注解，例如service层 @Service(value = \"accountService\") public class AccountServiceImpl implements AccountService { public void acc() { System.out.println(\"此方法是AccountService接口中的方法\"); } } 注解方式的依赖注入 @Autowired自动按照类型注入 可以用在变量上也可以用在方法上 假设有个变量是Service接口，那么有两个实现service的类，@Autowired就会先通过设置过的注解，自动找到两个类，然后通过变量名来识别要运行哪一个类 @Qualifier 作用 在按照类中的注入的基础之上再按照名称注入。它在给类成员注入时不能单独使用（必须和@Autowired进行配合）。但是在方法参数输入时可以 @Service(value = \"accountService\") public class AccountServiceImpl implements AccountService { @Autowired @Qualifier(\"userDao\") private UserDao userDao; public void acc() { System.out.println(\"此方法是AccountService接口中的方法\"); } } @Resource 可以直接指定Bean的id，但是属性值为name @Service(value = \"accountService\") public class AccountServiceImpl implements AccountService { @Resource(name=\"userDao\") private UserDao userDao; public void acc() { System.out.println(\"此方法是AccountService接口中的方法\"); } } 以上三种都只能注入其他bean类型的数据 @Value 用于注入基本类型和String数据 属性为value，可以用于指定数据的值。它可以使用spring中SpEL（也就是spring的el表达式）SpEL的写法：${表达式} @Scope 用于指定Bean的作用范围 属性为value，默认为单例 配置类注解的一些方法 @Configuration 指定当前类是一个配置类 @ComponentScan(basePackages=\"com.test\") 用于指定扫描的包 @Bean 用于把当前方法的返回值作为bean对象对如spring的ioc容器中 属性：name指定bean的id，当不写时，默认值为当前方法的名称 @Configuration @ComponentScan(basePackages=\"com.test\") public class SpringConfiguration(){ @Bean(name=\"runner\") public QueryRunner createQueryRunner(DataSource dataSource){ return new QueryRunner(dataSource); } @Bean(name=\"dataSource\") public DataSource createDataSource(){ //此方法为配置数据库连接信息 } } 如果使用配置类，获取容器的方法要用AnnotationConfigApplicationContext ApplicationContext as = new AnnotationConfigApplicationContext(\"SpringConfiguration.class\") AOP的概念 AOP（Aspect Oriented Programming）称为面向切面编程，在程序开发中主要用来解决一些系统层面上的问题，比如日志，事务，权限等待，Struts2的拦截器设计就是基于AOP的思想，是个比较经典的例子。 在不改变原有的逻辑的基础上，增加一些额外的功能。代理也是这个功能，读写分离也能用aop来做。 简单来说，它就是把我们程序可重复性的代码抽取出来，在需要执行的时候，使用动态代理的技术，在不修改源码的基础上，对我们已有的方法进行加强 基于XML方式的AOP 在pom.xml加入以下内容 org.springframework spring-context 5.2.3.RELEASE org.aspectj aspectjweaver 1.9.5 配置applicationConfig.xml（spring配置文件）文件 使用aop:config标签表明开始AOP的配置 使用aop:aspect标签表明配置切面 在aop:aspect中使用内部标签配置通知的类型 aop:before配置前置通知 method属性:指定aop:aspect指定的logger类中的对应方法 pointcut属性:用于指定切入点表达式，指的是对业务层的哪些方法进行加强 切入点表达式： execution(public void com.test.service.AccountServiceImpl.saveAccount()) execution( *.*.*.AccountServiceImpl.saveAccount()) execution( *..AccountServiceImpl.saveAccount()) execution( *..*.*()) 有参数的话，写类型就行 execution( *..*.*(..)) 全通配写法 pointcut-ref属性：用来指定切面表达式的id aop:after-returning配置后置通知类型 aop:after-throwing配置异常通知类型 aop:after配置最终通知 aop:around配置环绕通知 异常和后置只能执行一个 aop:around配置环绕通知 编写AccountServiceImpl中的aroundLogger方法(环绕通知) public Object aroundLogger(MethodInvocationProceedingJoinPoint pjp){ Object rtValue = null; try{ Object[] args = pjp.getArgs(); System.out.println(\"前置通知\"); rtValue = pjp.proceed(args); System.out.println(\"后置通知\"); return rtValue; }catch(Throwable e){ System.out.println(\"异常通知\"); throw new RuntimeException(e); }finally { System.out.println(\"最终通知\"); } } 基于注解的AOP 配置spring配置文件 设置切面类Logger @Aspect //表示当前类是一个切面类 @Component(\"logger\") @Aspect(\"execution(public void com.test.service.AccountServiceImpl.after())\") public class Logger{ @Pointcut private void pt1(){} //前置通知 @Before() private void before(){} //后置通知 @AfterReturning(\"pt1()\") private void afterReturning(){} //异常通知 @AfterThrowing(\"pt1()\") private void afterThrowing(){} //最终通知 @After() private void after(\"pt1()\"){} //环绕通知 @Around() private void around(\"pt1()\"{} } 前四个通知顺序会不一样，最终通知始终会在第三个 而Around环绕通知是按照顺序来的，按照你的顺序来写的 基于XML方式的声明式事务控制（推荐使用） 事务管理是企业级应用程序开发中必备技术，用来确保数据的完整性和一致性！ 防止插入数据和查询数据发生异常 避免了每次对数据操作都要现获得Session实例来启动事务/提交/回滚事务还有繁琐的Try /Catch操作 防止出现脏数据的一种方法，同时成功，或者同时失败 环境搭建 需要的Jar包：spring-jdbc,spring-tx,mysql-connector-java、aspectjweaver 在pom.xml中添加以下坐标： org.springframework spring-tx 5.2.3.RELEASE org.springframework spring-jdbc 5.2.3.RELEASE org.springframework spring-context 5.2.3.RELEASE} org.aspectj aspectjweaver 1.9.5 在Spring配置文件中添加以下内容： 配置事务的属性 isolation 用于指定事务的隔离级别，默认值是DEFAULT，表示使用数据库的默认隔离级别 propagation 用于指定事务的传播行为。默认值是REQUIRED,表示一定会有事务、增删改的选择，查询方法可以选择SUPPOETS read-only 用于指定事务是否只读。只有查询方法才能设置为true。默认值是false，表示读写 timeout 用于指定事务的超时时间，默认值是-1，表示永不超时。如果指定了数值，以秒为单位。 rollback-for 用于指定一个异常，当产生该异常时，事务回滚，产生其他异常时，事务不回滚。没有默认值。表示任何异常都回滚 no-rollback-for 用于指定一个异常，当产生该异常时，事务不回滚，产生其他异常时，事务回滚。没有默认值。表示任何异常都回滚 基于注解的声明式事务控制（麻烦） 如果一个类中有很多个增删改查方法，则需要配很多个事务注解 配置Spring配置文件 在需要事务支持的地方使用@Transactional注解 在类的上面或者方法的上面使用注解 "},"MyNotes/Java相关笔记/SpringMVC相关.html":{"url":"MyNotes/Java相关笔记/SpringMVC相关.html","title":"SpringMVC相关","keywords":"","body":"SpringMVC相关 SpringMVC概述 是一种基于Java实现的MVC设计模型的请求驱动类型的轻量级WEB框架 Spring MVC属于Spring框架的后续产品，已经融合在Spring Web Flow里面。 SpringMVC在三层架构属于WEB表现层 环境搭建（IDEA） 普通方式 Maven方式 在pom.xml中写入以下内容： 5.2.3.RELEASE org.springframework spring-context ${spring.version} org.springframework spring-web ${spring.version} org.springframework spring-webmvc ${spring.version} javax.servlet servlet-api 2.5 provided javax.servlet.jsp jsp-api 2.2 provided 以上为创建的方式 配置前端控制器（web.xml） 在web.xml中配置以下内容（加载SpringMVC配置文件） Archetype Created Web Application DispatcherServlet org.springframework.web.servlet.DispatcherServlet contextConfigLocation classpath:springMvc.xml 1 DispatcherServlet / 普通方式创建项目时，IDEA会自动帮你创建相应的配置文件（WEB文件夹中的WEB-INF下），并且web.xml中会有相应的初始值，只要适当的修改文件即可 创建SpringMVC配置文件（springmvc.xml） 普通方式创建：默认自动创建在web程序目录下的WEB-INF （dispatcher-servlet.xml和applicationContext.xml，分别代表springMVC配置文件和Spring文件） （普通方式java的程序目录只要src目录，没有main及以下的目录，必要时可手动创建） maven方式创建：在java程序根目录下的resource中创建配置文件 （Maven方式默认没有src/main/java和resource，需要手动配置） 添加以下内容： 视图解析器：配置指定的跳转页面的路径目录、文件后缀名 mvc:annotation-driven：自动加载处理器映射器和处理器适配器 利用view-controller标签进行请求地址和视图名称(jsp页面)进行关联 如果只创建controller并且跳转到某个页面，则不用写controller方法(节省时间)，直接在springmv配置文件中加入view-controller标签将请求地址和jsp页面进行绑定，一般用于后台页面跳转 path:controller方法名(请求名),例如：/admin/login、/admin/login.html view-name:视图名称 测试 创建index.jsp页面 href对应的内容前面如果加了 / 则是访问的根目录，不是web项目名的目录 入门程序 入门程序 跳转 创建success.jsp页面(webapp下的pages，pages需收到创建) 成功 成功。。。 创建Controller包（类路径下：java的程序目录下） package controller; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; //控制器类 @Controller public class HelloController { //映射的访问目录 @RequestMapping(\"/hello\") public String sayHello(){ System.out.println(\"Hello SpringMVC\"); return \"success\"; } } 配置Tomcat服务器 按照下面方式操作： 原理 RequestMapping注解介绍 作用 用于建立请求URL和处理请求方法之间的对应关系 可用在方法上面也可以用在类上面 @Controller @RequestMapping(\"/welcome\") public class HelloController { //映射的访问目录 @RequestMapping(\"/hello\") public String sayHello(){ System.out.println(\"Hello SpringMVC\"); return \"success\"; } } 用在类上面，则请求的url是/welcome/hello 属性 value和path都是指定映射的路径（不写则默认为value，一个参数的情况下） method用于指定请求类型（GET、POST） @RequestMapping(\"/hello\",method={RequestMethod.POST}) params用于指定限制请求参数的条件。它支持简单的表达式。要求请求的参数的key和value必须和配置的一模一样 @RequestMapping(\"/hello\",params=\"username\") @RequestMapping(\"/hello\",params=\"username=zh\") //请求的username和值必须要和上面一样 headers用于指定限制请求消息头的条件 发送的请求中必须包含的请求头 @RequestMapping(\"/hello\",params=\"username\",headers={\"Accept\"}) 请求参数绑定 请求参数绑定字符串 @RequestMapping(\"/testParam\") public String testParam(String username,String password){ System.out.println(\"执行了...\"); System.out.println(\"用户名：\"+username); return \"success\"; } //如果请求的参数为username,password，则可以在方法后面添加相对应的参数名即可获取参数 请求参数绑定实体类 姓名： 密码： 金额： 用户的姓名： 用户的年龄： 如果Account实体类中含有User实体类，则name应写成：user.uname @RequestMapping(\"/saveAccount\") public String saveAccount(Account account){ System.out.println(account); } 配置解决中文乱码的过滤器 在web.xml添加过滤器 charset org.springframework.web.filter.CharacterEncodingFilter encoding utf-8 charset /* 请求参数绑定集合类型 姓名： 密码： 金额： 封装到List中 用户的姓名： 用户的年龄： 封装到map中 用户的姓名： 用户的年龄： 自定义类型转换器 写一个类实现Converter接口 Converter 是你要从什么转成什么 package utils; import org.springframework.core.convert.converter.Converter; import java.text.ParseException; import java.text.SimpleDateFormat; import java.util.Date; public class StringToDateConverter implements Converter { @Override public Date convert(String s) { if (s == null){ throw new RuntimeException(\"请您传入数据\"); } SimpleDateFormat df = new SimpleDateFormat(\"yyyy-MM-dd\"); try { return df.parse(s); } catch (ParseException e) { throw new RuntimeException(\"数据类型转换出现错误\"); } } } 配置自定义类型转换器 在springmvc配置文件中配置 获取Servlet原生的API @RequestMapping(\"/testServlet\") public String testServlet(HttpServletRequest request, HttpServletResponse response){ System.out.println(request); HttpSession session = request.getSession(); System.out.println(session); ServletContext servletContext = session.getServletContext(); System.out.println(servletContext); System.out.println(response); return \"success\"; } 常用注解 RequestParam注解 作用 把请求中指定名称的参数给控制器中的形参赋值 RequestParam @RequestMapping(\"/testRequestParam\") private String testRequestParam(@RequestParam(\"name\") String username,@RequestParam(\"pass\") String password){ System.out.println(username); System.out.println(password); return \"success\"; } RequestBody注解 作用 用于获取请求体内容。直接使用得到是key=value&key=value...结构的数据。 get请求方式不适用 RequestBody @RequestMapping(\"/testRequestBody\") private String testRequestBody(@RequestBody String body){ System.out.println(body); return \"success\"; } //得到结果name=哈哈&pass=123 PathVariable注解 作用 用于绑定url中的占位符。例如：请求url中/delete/{id},这个{id}就是url占位符。 url支持占位符是spring3.0之后加入的。是springmvc支持rest风格URL的一个重要标志。 @RequestMapping(\"/testPathVariable/{sid}\") private String testPathVariable(@PathVariable(name=\"sid\") String id){ System.out.println(id); return \"success\"; } ModelAttribute注解 作用 该注解是Spring'MVC4.3版本之后新加入的。它可以用于修饰方法和参数。 出现在方法上，表示当前方法会在控制器的方法执行之前，先执行。它可以修饰没有返回值的方法。也可以修饰有具体返回值的方法 出现在参数上，获取指定的数据给参数赋值 @RequestMapping(\"/testModelAttribute\") private String testModelAttribute(){ System.out.println(id); return \"success\"; } @ModelAttribute public void showUser(){ System.out.println(\"showUser执行了\") } //先执行showUser再执行testModelAttribute SessionAttribute注解及存值到域对象中 作用 用于多次执行控制器方法间的参数共享 SessionAttribute @RequestMapping(\"/anno\") @SessionAttributes(value = \"msg\") //把msg=王五存到session域中 public class AnnoCotroller { @RequestMapping(\"/testSessionAttribute\") private String testSessionAttribute(Model model){ System.out.println(\"testPathVariable执行了\"); //底层会存储到request域对象中 model.addAttribute(\"msg\",\"王五\"); return \"success\"; } } 在success.jsp中取值 成功 成功。。。 ${requestScope.msg} ${sessionScope.msg} 从JSP页面中的域对象取值 @RequestMapping(\"/getSessionAttribute\") private String getSessionAttribute(ModelMap modelMap){ System.out.println(\"getSessionAttribute执行了\"); //底层会取出request域对象中的数据 String msg = (String) modelMap.get(\"msg\"); System.out.println(msg); return \"success\"; } ModelAndView类型响应返回值（常用） @RequestMapping(\"/testModelAndView\") private ModelAndView testModelAndView(){ ModelAndView mv = new ModelAndView(); System.out.println(\"testModelAndView执行了\"); //把对象值存到mv中，也会存到request域中 mv.addObject(\"msg\",\"王五\"); //跳转到哪个页面,会去找视图解析器 mv.setViewName(\"success\"); return mv; } forward和redirect进行转发和重定向（很少用） @RequestMapping(\"/testForwardOrRedirect\") private String testForwardOrRedirect(){ //这两种方法不会经过视图解析器 //转发操作 //return \"forward:/WEB-INF/pages/success.jsp\"; //重定向操作 return \"redirect:/pages/success.jsp\" } ResponseBody响应json数据（常用） $(function () { $(\"#btn\").click(function () { $.ajax({ type:\"post\", url:\"user/testAjax\", contentType:\"application/json;charset=UTF-8\", data:'{\"username\":\"haha\"}', dataType:\"json\", success:function (data) { // data服务器响应的json数据，进行解析 alert(\"用户名:\"+data.username+\"密码：\"+data.password+\"年龄：\"+data.age); } }) }) }) 因需要用到json解析，所以需要jackson的jar包 普通方式需手动下载：https://maven.aliyun.com/mvn/search Maven方式需要在pom.xml添加以下坐标： com.fasterxml.jackson.core jackson-annotations 2.10.2 com.fasterxml.jackson.core jackson-core 2.10.2 com.fasterxml.jackson.core jackson-databind 2.10.2 package controller; import entity.User; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestBody; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.ResponseBody; @Controller @RequestMapping(\"/user\") public class UserController { /** * 模拟异步请求响应 */ @RequestMapping(\"/testAjax\") @ResponseBody public User testAjax(@RequestBody User user){ //客户端发送ajax的请求，传的是json字符串，后端把json字符串封装到user对象中 System.out.println(user); //做响应,模拟查询数据库 user.setUsername(\"haha\"); user.setPassword(\"1223\"); user.setAge(18); //做响应，返回User对象 return user; } } MappingJackson2JsonView方法返回json数据页面 https://blog.csdn.net/hehyyoulan/article/details/84779402 @RequestMapping(value = \"/users\") public ModelAndView users(){ User user = new User(); user.setUsername(\"haha\"); user.setPassword(\"1223\"); user.setAge(18); ModelAndView mav=new ModelAndView(new MappingJackson2JsonView()); mav.addObject(user); return mav; } SpringMVC下的ajax请求 当ajax里面没有定义Content-type的时候，jquery默认使用application/x-www-form-urlencoded类型。 用@RequestParam接收前端的数据 当我们在后端用@RequestParam来接收数据时，是支持接收application/x-www-form-urlencoded格式传输的JSON对象。意思就是说后端可以接收数据 js代码 $(function () { //此处的data为json对象，在js打印出来则返回[object Object],并不会返回对应的内容 var data = {\"username\":\"zhangsan\",\"password\":\"123456\"} $(\"#btn\").click(function () { $.ajax({ type:\"post\", url:\"user/testAjax1\", contentType:\"application/json;charset=UTF-8\", data:data, dataType:\"json\", success:function (data) { // data服务器响应的json数据，进行解析 alert(\"用户名:\"+data.username+\"密码：\"+data.password+\"年龄：\"+data.age); } }) }) }) ​ SpringMVC代码 能接收到数据 @RequestMapping(\"/testAjax1\") private String testRequestParam(@RequestParam(\"username\") String username,@RequestParam(\"password\") String password){ System.out.println(username); System.out.println(password); return \"success\"; } 用@RequestBody接收前端的数据 但是，当我们在后端用@RequestBody来接收数据时，Content-type的类型必须设置为application/json,不会解析JSON对象。而我们下方的data是JSON对象，所以我们要将JSON对象转成JSON字符串，得到JSON对象中的内容。并且发送给后端。 利用JSON.stringify()来将JSON对象转成JSON字符串 ​ js代码 所以，如果我们使用上方ajax代码，则后端不会接收到数据，并且前端会报错误 $(function () { //此处的data为json对象，在js打印出来则返回[object Object],并不会返回对应的内容 var data = {\"username\":\"zhangsan\",\"password\":\"123456\"}; $(\"#btn\").click(function () { $.ajax({ type:\"post\", url:\"user/testAjax1\", data:JSON.stringify(data), //将JSON对象转成JSON字符串 dataType:\"json\", success:function (data) { // data服务器响应的json数据，进行解析 alert(\"用户名:\"+data.username+\"密码：\"+data.password+\"年龄：\"+data.age); } }) }) }) ​ SpringMVC代码 @RequestMapping(\"/testAjax\") public User testAjax(@RequestBody User user){ //客户端发送ajax的请求，传的是json字符串，后端把json字符串封装到user对象中 System.out.println(user); return \"success\"; } } 编写统一JSON格式工具类 我们需要创建一个util包，这个包中来写工具来 创建JsonMSG工具类 package com.memorygzs.util; import java.util.HashMap; import java.util.Map; public class JsonMSG { //状态码 1成功 0失败 private int code; //提示信息 private String msg; //要返回的数据 private Map data = new HashMap(); /** * 请求成功,但是不返回数据 * @return */ public static JsonMSG success(){ JsonMSG jsonMSG = new JsonMSG(); jsonMSG.setCode(1); jsonMSG.setMsg(\"ok\"); return jsonMSG; } /** * 请求失败，不返回数据 * @return */ public static JsonMSG fail(){ JsonMSG jsonMSG = new JsonMSG(); jsonMSG.setCode(0); jsonMSG.setMsg(\"fail\"); return jsonMSG; } public JsonMSG addInfo(String key, Object value){ this.getData().put(key,value); return this; } public int getCode() { return code; } public void setCode(int code) { this.code = code; } public String getMsg() { return msg; } public void setMsg(String msg) { this.msg = msg; } public Map getData() { return data; } public void setData(Map data) { this.data = data; } } 在Controller中返回JsonMSG类型 @RequestMapping(\"/findAll.json\") @ResponseBody public JsonMSG findAllJson(){ List list = new List; //返回成功的状态码和信息以及data数据 return JsonMSG.success().addInfo(\"listInfo\",list); } 最终的数据为: {\"code\":1,\"msg\":\"ok\",\"data\":{\"listInfo\":[\"a\",\"b\",\"c\"]}} 文件上传 SpringMVC文件上传需要以下依赖包 maven方式在pom.xml中加入下面坐标 commons-fileupload commons-fileupload 1.4 commons-io commons-io 2.6 传统方式 利用Servelt中request，文件上传工具中的DiskFileItemFactory（设置用于储存的临时路径）和ServletFileUpload（一个上传工具，用于上传的） //enctype必须设置为multipart/form-data，表示是一个文件上传的表单，而不是普通表单 选择文件： private String fileupload1(HttpServletRequest request) throws Exception { System.out.println(\"文件上传....\"); //上传的位置 String path = request.getSession().getServletContext().getRealPath(\"/uploads/\"); //判断该路径是否存在 File file = new File(path); if (!file.exists()){ //创建该文件夹 file.mkdirs(); } DiskFileItemFactory factory = new DiskFileItemFactory(); //创建一个上传工具，指定使用缓存区与临时文件存储位置. ServletFileUpload upload = new ServletFileUpload(factory); //解析request,获取所有的上传文件项，FileItem就是一个上传项 List items = upload.parseRequest(request); for (FileItem item:items) { if (item.isFormField()){ //为普通表单 }else { //为上传文件项 //获取上传文件的名称 String filename = item.getName(); //把文件的名称设置唯一值，uuid String uuid = UUID.randomUUID().toString().replace(\"-\",\"\"); filename = uuid+\"_\"+filename; //上传控件，将上传项保存到服务器中 item.write(new File(path,filename)); //删除临时文件 item.delete(); } } return \"success\"; } SpringMVC文件上传 //enctype必须设置为multipart/form-data，表示是一个文件上传的表单，而不是普通表单 //input中name必须和controller中MultipartFile upload 参数一样的名字 选择文件： 在springmvc配置文件中配置文件解析器 如果加了以下解析器，则传统方式不可用，会被拦截 原因：因为在springmvc配置文件中已经将request解析为MultipartHttpServletRequest，所以在传统方式下如果再次解析（二次解析）则返回的值会为空。 写Controller方法 /** * springmvc文件上传 * @param upload * @return */ @RequestMapping(\"/fileupload2\") private String fileupload2(HttpServletRequest request,MultipartFile upload) throws IOException { System.out.println(\"SpringMVC文件上传....\"); //上传的位置 String path = request.getSession().getServletContext().getRealPath(\"/uploads/\"); //判断该路径是否存在 File file = new File(path); if (!file.exists()){ //创建该文件夹 file.mkdirs(); } String filename = upload.getOriginalFilename(); //把文件的名称设置唯一值，uuid String uuid = UUID.randomUUID().toString().replace(\"-\",\"\"); filename = uuid+\"_\"+filename; //上传控件，将上传项保存到服务器中 upload.transferTo(new File(path,filename)); return \"success\"; } 跨服务器上传 跨服务器上传需要的依赖包：jersey-client、jersy-core maven方式在pom.xml添加以下坐标： com.sun.jersey jersey-core 1.19.4 com.sun.jersey jersey-client 1.19.4 写Controller方法 /** * 跨服务器文件上传 * @param upload * @return */ @RequestMapping(\"/fileupload3\") private String fileupload3(MultipartFile upload) throws IOException { System.out.println(\"跨服务器文件上传....\"); //定义文件上传服务器的路径 String path = \"http://localhost:9090/uploads/\"; String filename = upload.getOriginalFilename(); //把文件的名称设置唯一值，uuid String uuid = UUID.randomUUID().toString().replace(\"-\",\"\"); filename = uuid+\"_\"+filename; //创建客户端对象 Client client = Client.create(); //和文件服务器进行连接 WebResource webResource = client.resource(path + filename); //上传文件,通过字节的方式上传 webResource.put(upload.getBytes()); return \"success\"; } 报错：com.sun.jersey.api.client.UniformInterfaceException: PUT http://localhost:9090/uploads/654a1661b41149cea4c920f8e5734970.jpg returned a response status of 405 Method Not Allowed 原因：TOMCAT考虑到安全性，默认关闭了TOMCAT的PUT和DELETE请求(即readonly = true) 解决方法：进入到文件服务器的tomcat的config目录下，修改web.xml,在default servlet下增加参数readonly=false 路径：E:\\apache-tomcat-9.0.22-windows-x64\\apache-tomcat-9.0.22\\conf\\web.xml default org.apache.catalina.servlets.DefaultServlet debug 0 listings false readonly false 1 SpringMVC的异常处理 编写自定义异常类（做提示信息的） package exception; /** * 自定义异常类 **/ public class SysException extends Exception { //存储提示信息的 private String message; public SysException(String message) { this.message = message; } public String getMessage() { return message; } public void setMessage(String message) { this.message = message; } } 在controller中模拟异常发生 package controller; import exception.SysException; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; @Controller @RequestMapping(\"/user\") public class UserController { @RequestMapping(\"/testException\") private String testException() throws SysException { System.out.println(\"testException执行了...\"); try { //模拟异常 int a = 10/0; } catch (Exception e) { e.printStackTrace(); //抛出自定义异常信息 throw new SysException(\"查询所有用户出现错误了...\"); } return \"success\"; } } 编写异常处理器 进行异常处理器的编写，处理异常业务逻辑 要编写SpringMVC的异常处理器需要实现HandlerExceptionResolver接口里面的方法 方法返回ModelAndView类型，在方法中可以将自定义异常信息传给前端 package exception; import org.springframework.web.servlet.HandlerExceptionResolver; import org.springframework.web.servlet.ModelAndView; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; /** * 处理异常业务逻辑 **/ public class SysExceptionResolver implements HandlerExceptionResolver { @Override public ModelAndView resolveException(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, Exception ex) { //获取异常对象 SysException e = null; if (ex instanceof SysException){ e = (SysException)ex; }else { e = new SysException(\"系统正在维护\"); } //创建ModelAndView对象 ModelAndView mv = new ModelAndView(); mv.addObject(\"errorMSG\",e.getMessage()); mv.setViewName(\"error\"); return mv; } } 配置异常处理器（跳转到提示页面） 在springmvc配置文件中配置自己写的异常处理器（可当做普通Bean来配） SpringMVC的拦截器 拦截器和过滤器的区别 过滤器是servlet规范中的一部分，任何java web工程都可以使用 拦截器是SpringMVC框架自己的，只有使用了SpringMVC框架的工程才能使用 过滤器在url-pattern中配置了/*之后，可以对所有要访问的资源拦截 拦截器它只会拦截访问的控制器（Controller）方法，如果访问的是jsp，html，css，image或者js是不会拦截的 编写拦截器类，实现HandlerInterceptor接口 重写方法 package interceptor; import org.springframework.web.servlet.HandlerInterceptor; import org.springframework.web.servlet.ModelAndView; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; /** * 自定义拦截器 **/ public class MyInterceptor1 implements HandlerInterceptor { /** * 预处理，controller方法前执行 * return true放行，执行下一个拦截器，如果没有，执行controller中的方法 * return false不放行 */ @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { System.out.println(\"MyInterceptor1执行了...前\"); return true; } /** *后处理方法 */ @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception { System.out.println(\"MyInterceptor1执行了...后\"); } /** *当后处理方法执行完返回到页面时，最后再执行方法 */ @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { System.out.println(\"MyInterceptor1执行了...最后\"); } } 配置拦截器 在springmvc配置文件配置拦截器 --> --> "},"MyNotes/Java相关笔记/Mybatis相关.html":{"url":"MyNotes/Java相关笔记/Mybatis相关.html","title":"Mybatis相关","keywords":"","body":"Mybatis相关 Mybatis环境搭建(IDEA) 新建IDEA项目 添加以下内容到pom.xml org.mybatis mybatis 3.5.4 mysql mysql-connector-java 8.0.19 创建User实体类 创建entity包,在entity里面创建User实体类 package entity; import java.util.Date; public class User { private Integer id; private String username; private Date birthday; private String address; public Integer getId() { return id; } public void setId(Integer id) { this.id = id; } public String getUsername() { return username; } public void setUsername(String username) { this.username = username; } public Date getBirthday() { return birthday; } public void setBirthday(Date birthday) { this.birthday = birthday; } public String getAddress() { return address; } public void setAddress(String address) { this.address = address; } @Override public String toString() { return \"User{\" + \"id=\" + id + \", username='\" + username + '\\'' + \", birthday=\" + birthday + \", address='\" + address + '\\'' + '}'; } } 创建IUserDao接口 创建dao包，在dao里面创建IUserDao接口 package dao; import entity.User; import java.util.List; /** * 用户的持久层接口 **/ public interface IUserDao { /** * 查询所有操作 * @return */ List findAll(); } 在resources文件夹中创建SqlMapConfig.xml文件（Mybatis的主配置文件） 创建Mybatis的映射配置文件 在resources文件夹中创建mapper文件夹再创建IUserDao.xml添加以下内容: select * from user 注意：完成以上操作，无需创建接口的实现类 创建Main类（用来测试mybatis） import dao.IUserDao; import entity.User; import org.apache.ibatis.io.Resources; import org.apache.ibatis.session.SqlSession; import org.apache.ibatis.session.SqlSessionFactory; import org.apache.ibatis.session.SqlSessionFactoryBuilder; import java.io.InputStream; import java.util.List; public class Main { public static void main(String[] args) throws Exception { //1.读取配置文件 InputStream in = Resources.getResourceAsStream(\"SqlMapConfig.xml\"); //2.创建SqlSessionFactory工厂 /** * 因SqlSessionFactory是一个接口，所以要用 * SqlSessionFactoryBuilder来创建一个SqlSessionFactory对象 */ SqlSessionFactoryBuilder builder = new SqlSessionFactoryBuilder(); SqlSessionFactory factory = builder.build(in); //3.创建工厂生产SqlSession对象 SqlSession sqlSession = factory.openSession(); //4.使用SqlSession创建Dao接口的代理对象 IUserDao mapper = sqlSession.getMapper(IUserDao.class); //5.使用代理对象执行方法 List users = mapper.findAll(); for (User user:users){ System.out.println(user); } //6.释放资源 sqlSession.close(); in.close(); } } Mybatis的XML增删改查 增删改测试时都需要提交并且要实例化User实体类 还要给User中的属性设置值 注解的方式是用@+要执行的增删改查方法，后面再加上下方的sql语句 例：@Update(\"update user set username=#{username} where id=#{id}\") 增加 insert into user(username,address,sex,birthday) values(#{username},#{address},#{sex},#{birthday}) 删除 delete from user where address=#{address} 更新 update user set username=#{username} where id=#{id} 查询 select username from user select * from user where id=#{id} select * from user where username like #{username} Mybatis中的XML多表查询 一对一 人和身份证号 ​ 一个人只能有一个身份证号 ​ 一个身份证号对应一个人 新建Person实体类 一对一：随便在哪个实体类中添加另一个实体类和id，这里用Person类中包含Card类和id 字段: pid、username(姓名)、age(年龄)、sex(性别)、cid(身份证id) private Integer pid; private String username; private Integer age; private Integer sex; private Integer cid; private Card card; 新建Card实体类 字段： cid、card(身份证号) private Integer cid; private String card; 新建IPersonDao接口 package dao; import entity.Person; import java.util.List; public interface IPersonDao { /** * 通过pid查询出身份证号 **/ List findAll(); } 新建IPersonDao.xml映射文件 SELECT * from person p,card c where p.cid=c.cid 在Mybatis主配置文件中添加对应的映射文件 测试 一对多 用户和账户 ​ 一个用户可以有多个账户 新建User实体类 //一对多关系映射:主表实体（User）应该包含从表实体（Account）的集合引用 private Integer id; private String username; private String password; private Date birthday; private String sex; private String address; private List accounts; 新建Account实体类 在多的一方添加User表中的id private Integer id; //账户id private Double money; private Integer uid; //用户id 新建IUserDao接口 package dao; import entity.User; import java.util.List; public interface IUserDao { /** * 通过用户id去查询该用户有多少个账户 **/ List findAll(); } 新建IUserDao.xml映射文件 SELECT u.*,a.id AS aid,a.`money` FROM USER u LEFT JOIN account a ON u.`id` = a.`uid` 在主配置文件中添加对应的映射文件 测试 多对多 和一对多的配置是一样的，只是在两个配置文件中都要配 学生和老师 ​ 一个学生可以被多名老师教 ​ 一个老师可以教多名学生 多对多关系：在双方实体类中添加对方的list集合 需要在数据库中创建3张表:student表、teacher表、student_teacher表（中间表） 新建Student实体类 字段 sid、sname(学生姓名)、s_sex(学生性别) private Integer studentId; private String studentName; private String studentSex; //多对多关系映射:一个学生被多名老师教 private List teachers; 新建Teacher实体类 字段 tid、tnamet(老师姓名)、t_sex(老师性别) private Integer teacherId; private String teacherName; private String teacherSex; //多对多关系映射:一个老师有多名学生 private List students; 新建IStudentDao接口 package dao; import entity.Student; import java.util.List; public interface IStudentDao { /** * 查询学生，以及学生有多少个老师 **/ List findAll(); } 新建ITeacherDao接口 package dao; import entity.Teacher; import java.util.List; public interface ITeacherDao { /** * 查询老师，以及老师有多少个学生 **/ List findAll(); } 新建IStudentDao.xml映射文件 select s.*,t.* from student s left join student_teacher st on s.sid = st.sid left join teacher t on t.tid = st.tid 新建ITeacherDao.xml映射文件 select t.*,s.* from teacher t left join student_teacher st on t.tid = st.tid left join student s on s.sid = st.sid 在主配置文件中添加对应的映射文件 测试 Mybatis注解的使用 新建IDEA项目 添加内容到pom.xml 修改SqlMapConfig文件 把上面SqlMapConfig文件内容复制一下，修改以下内容： 注意：修改完之后Resources文件夹里面的mapper文件夹（映射配置文件）可以 删除，在使用注解方式的时候映射文件是没有用的 创建User实体类 创建IUserDao接口 public interface IUserDao { @Select(\"select * from user\") List findAll(); } Mybatis注解建立实体类数据库表对应关系 id=\"userMap\"可以让别的方法引用： 例： @Select(\"select * from user\") @Results(value={\"userMap\"}) 或者 @Results(\"userMap\") public interface IUserDao { @Select(\"select * from user\") @Results(id=\"userMap\",value={ @Result(id=true,column = \"id\",property = \"userid\"), @Result(column = \"username\",property = \"userName\"), @Result(column = \"sex\",property = \"sex\") }) List findAll(); } Mybatis注解多表查询 一对一 public interface IUserDao { @Select(\"select * from user\") @Results(id=\"userMap\",value={ @Result(id=true,column = \"id\",property = \"userid\"), @Result(column = \"username\",property = \"userName\"), @Result(column = \"sex\",property = \"sex\")， @Result(property = \"user\",column = \"uid\",one=@one(select=\"dao.IUserDao.findByid\",fetchType = FetchType.EAGER)) }) List findAll(); } 一对多 @Result(property = \"accounts\",column = \"id\",many=@many(select=\"dao.IAccountDao.findAccountByid\",fetchType = FetchType.LAZY)) }) Mybatis主配置文件标签信息 Properties标签的使用及细节 将数据库连接信息写入到resources文件夹中的jdbcConfig.properties文件中 driver=com.mysql.cj.jdbc.Driver url=jdbc:mysql://localhost:3306/db_mybatis?serverTimezone=Asia/Shanghai user=root password=root 修改Mybatis主配置文件中的连接池信息 #在configuration标签下添加以下标签（引入jdbcConfig.properties）： #修改连接池信息（value和jdbcConfig.properties文件中的内容相对应): TypeAliases标签和Package标签 TypeAliases标签（双标签） 放在environments标签之上 标签内可以放typeAlias标签，用于配置别名，type属性指定的是实体类全限定类名。alias属性指定别名，当指定了别名不区分大小写 Package标签 放在typeAliases标签中是用于配置别名的包，name属性为指定的包路径。该包下的实体类都会注册别名，并且类名就是别名，不区分大小写 放在mappers标签中是用于指定dao接口所在的包，不需要写class或者resource Mybatis中动态SQL语句 If标签 where 1=1 表示无条件的，当用户查询信息的时候，如果没有选择任何选项也不会出错，当你选择了哪些信息，就要在where 1=1 后面加上 and + 你选择的字段信息 select * from user where 1=1 and username = #{username} and sex = #{sex} and sex = #{address} where 标签 使用了where标签就不需要使用where 1=1 select * from user and username = #{username} and sex = #{sex} and sex = #{address} bind标签 bind 标签可以使用 OGNL 表达式创建一个变量井将其绑定到上下文中。用于模糊查询 bind 标签的两个属性都是必选项， name 为绑定到上下文的变量名， va l ue 为 OGNL 表 达式。 select * from user where name like #{username} value中的name为传入的字段属性名，也可以设置为 _parameter.getName() #{username} 必须为bind中的name foreach标签 foreach的主要用在构建in条件中，它可以在SQL语句中进行迭代一个集合。 ​ 属性： ​ item：集合中元素迭代时的别名，该参数为必选。 ​ index：在list和数组中,index是元素的序号，在map中，index是元素的key，该参数可选 ​ open：foreach代码的开始符号，一般是(和close=\")\"合用。常用在in(),values()时。该参数可选 ​ separator：元素之间的分隔符，例如在in()的时候，separator=\",\"会自动在元素中间用“,“隔开，避免手动输入 逗号导致sql错误，如in(1,2,)这样。该参数可选。 ​ close: foreach代码的关闭符号，一般是)和open=\"(\"合用。常用在in(),values()时。该参数可选。 ​ collection: 要做foreach的对象，作为入参时，List对象默认用\"list\"代替作为键，数组对象有\"array\"代替作为 键，Map对象没有默认的键。当然在作为入参时可以使用@Param(\"keyName\")来设置键，设置keyName 后，list,array将会失效。 除了入参这种情况外，还有一种作为参数对象的某个字段的时候。举个例子：如果 User有属性List ids。入参是User对象，那么这个collection = \"ids\".如果User有属性Ids ids;其中Ids是个 对象，Ids有个属性List id;入参是User对象，那么collection = \"ids.id\" 单参数List类型： select count(*) from users id in #{item.id} 单参数Arrays数组类型： select * from t_blog where id in #{item} Map类型： List ids = new ArrayList() ids.add(1); ids.add(2); ids.add(3); Map params = new HashMap(); params.put(\"ids\",ids) 下面的ids是传入的参数map的key值： select * from t_blog where title like \"%\"#{title}\"%\" and id in #{item} PageHelper分页插件 配置 在pom.xml中添加pagehelper依赖坐标 com.github.pagehelper pagehelper 5.1.11 在SqlSessionFactoryBean配置MyBatis插件(Spring.xml) 在相应的位置添加以下内容: true PageHelper插件4.0.0以后的版本支持自动识别使用的数据库 所以不需要配置 \\mysql\\ 使用 dao层写分页sql语句时不需要添加limit SELECT * FROM goods WHERE gname like concat(\"%\",#{gname},\"%\") ​ dao接口中返回List集合 List queryByGName(String gname); 在service层使用PageHelper 要传入以下参数: 1.要查询的字符串-gname 2.要查询的分页页码-pageNum 3.设置每一页有多少条数据-pageSize service接口中添加以下方法，返回PageInfo类型的集合 PageInfo getPageInfo(String gname,Integer pageNum,Integer pageSize); ​ service实现类 @Override public PageInfo getPageInfo(String gname, Integer pageNum, Integer pageSize) { //1.调用PageHelper的静态方法开启分页功能 //这里充分体现了PageHelper的“非侵入式”设计:原本要做的查询不必有任何修改 PageHelper.startPage(pageNum,pageSize); //2.执行查询 List list = iGoodsTypeDao.queryByGName(gname); //3.封装到PageInfo对象中 return new PageInfo<>(list); } Controller层 PageInfo pageInfoByName = iArtService.getPageInfoByName(name, page, 10); modelAndView.addObject(\"pageInfo\",pageInfoByName.getList()); //pageInfoByName.getList() 获取list对象，也就是查询出来的List "},"MyNotes/Java相关笔记/SpringBoot相关.html":{"url":"MyNotes/Java相关笔记/SpringBoot相关.html","title":"SpringBoot相关","keywords":"","body":"SpringBoot相关 SpringBoot简介 简化Spring应用开发的框架； 整个Spring技术栈的一个大整合; J2EE开发的一站式解决方案; 微服务 2014年Martin Fowler提出 Martin Fowler个人博客 微服务是一种架构风格 一个应用应该是一组小型服务;可以通过HTTP的方式进行互通; 每一个功能元素(服务)最终都是一个可独立替换和独立升级的软件单元 详细请参考Martin Fowler微服务文档 环境搭建(IDEA) 通过官网创建 如果选择Maven，则相当于在IDEA中创建Maven项目 打开官方网址:https://start.spring.io/ 将下载的项目导入到IDEA中 打开pom.xml文件，然后IDEA自动加载坐标 通过IDEA的脚手架工具创建(Spring Initializr) 推荐 从spring官网创建，相当于方法一的官网创建移到IDEA中来创建 剩下的操作和上面一样的 Maven方式 在pom.xml添加以下内容: org.springframework.boot spring-boot-starter-parent 2.2.4.RELEASE org.springframework.boot spring-boot-starter-web org.springframework.boot spring-boot-maven-plugin 编写SpringBoot主程序 方法①和方法②已经创建好主程序类 在main/java下创建包并且创建SpringBoot主程序类 import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; /** * @SpringBootApplication 来标注一个主程序类，说明这是一个Spring Boot应用 **/ @SpringBootApplication public class SpringBootMainApplication { public static void main(String[] args) { //使SpringBoot应用启动起来,下面第一个参数必须是一个SpringBoot的主程序类 SpringApplication.run(SpringBootMainApplication.class,args); } } 编写Controller 创建并且编写Controller类 package com.memorygzs.controller; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.ResponseBody; //@ResponseBody //@Controller @RestController public class HelloController { @RequestMapping(\"/hello\") public String hello(){ return \"Hello World\"; } } @RestController可以代替@ResponseBody和@Controller,因为@RestController包含这两个注解 启动SpringBoot程序 在刚刚写的SpringBoot主程序类中启动Main方法就可以启动SpringBoot应用了 从上图可以看出，在8080端口启动，访问的时候要带8080进行测试 为什么是8080端口？因为SpringBoot中内置tomcat，所以我们并不需要配置tomcat 部署到服务器中 如果pom.xml中没有配置以下插件，则不会打包springboot中的jar包 在idea最右边找到Mavem选项卡，进行下面操作 打包位置放到以下目录 将此jar包放到服务器中，使用以下命令启动 我是用cmd进行操作 输入命令: java -jar SpringBoot_01-1.0-SNAPSHOT.jar SpringBoot配置 配置文件 SpringBoot使用一个全局的配置文件，配置文件名是固定的；以下两种格式任选一种 application.properties 优先级高 application.yml 配置文件的作用:修改SpringBoot自动配置的默认值 YAML配置文件 以数据为中心，比json、xml等更适合做配置文件 ​ 写法为下面内容: server: port: 8081 语法 k:(空格)v:表示一对键值对(空格必须有) 以空格的缩进来控制层级关系，只要是左对齐的一列数据，都是同一级的 属性和值也是大小写敏感 server: port: 801 path: /hello 值的写法 字面量:普通的值(数字，字符串，布尔) 字符串默认不用家伙是那个单引号或者双引号，如果用了，双引号不会转义字符、单引号会转义字符 name: \"zhangsan \\n lisi\" 输出：zhangsan 回车 lisi 双引号 name: 'zhangsan \\n lisi' 输出：zhangsan \\n lisi 单引号 对象、Map（属性和值）(键值对) k:v friends: lastName: zhangsan age: 20 ​ 行内写法: friends:{lastName: zhangsan,age: 10} 数组（List、set） 用(空格)-(空格)值表示数组中的一个元素 pets: - cat - dog - pig ​ 行内写法 pets:[cat,dog,pig] 配置文件注入值 yml和properties文件任选一个，如果两个同时存在，会选properties application.properties person.lastName=张三 person.age=18 person.boss=false person.birth=2020/1/3 person.maps.k1=v1 person.maps.k2=v2 person.lists=a,b,c person.dog.name=小狗 person.dog.age=2 乱码请设置properties默认编码为UTF-8 application.yml文件 这个对应着Person实体类中的属性及值 person: lastName: zhangsan age: 18 boss: false birth: 2020/1/3 maps: {k1: v1,k2: v2} lists: [a,b,c] dog: name: 小狗 age: 2 Person实体类 要在实体类中用@Component,才能提供@ConfigurationProperties功能 要使用@ConfigurationProperties注解，需要在pom.xml中配置以下坐标: org.springframework.boot spring-boot-configuration-processor true package com.memorygzs.bean; import org.springframework.boot.context.properties.ConfigurationProperties; import org.springframework.stereotype.Component; import java.util.Date; import java.util.List; import java.util.Map; /** * 将配置文件中配置的每一个属性的值，映射到这个组件中 * @ConfigurationProperties 告诉SpringBoot将本类中所有的属性和配置文件中相关的配置进行绑定 * prefix:\"person\" 配置文件中哪个下面的所有属性和本类进行一一映射 * 这个类必须是Spring容器中一个组件，才能提供@ConfigurationProperties功能 * 所以要在下面添加@Component */ @Component @ConfigurationProperties(prefix = \"person\") public class Person { private String lastName; private Integer age; private Boolean boss; private Date birth; private Map maps; private List lists; private Dog dog; //getter、setter、toString方法这里省略 } @ConfigurationProperties 告诉SpringBoot将本类中所有的属性和配置文件中相关的配置进行绑定 prefix:\"person\" 配置文件中哪个下面的所有属性和本类进行一一映射 @PropertySource和@ImportResource注解(用于加载指定的配置文件) @PropertySource:加载指定的配置文件 @PropertySource(value = {\"classpath:person.properties\"}) @Component @ConfigurationProperties(prefix = \"person\") public class Person { private String lastName; private Integer age; private Boolean boss; private Date birth; } @ImportResource:导入Srping的配置文件，让配置文件里面的内容生效 SpringBoot里面没有Spring的配置文件，我们自己编写的配置文件也不能识别 想让Spring的配置文件生效，加载进来，需要将@ImportResource标注在SpringBoot主配置类上 import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.context.annotation.PropertySource; @PropertySource(value = {\"classpath:person.properties\"}) @SpringBootApplication public class SpringBootMainApplication { public static void main(String[] args) { //使SpringBoot应用启动起来 SpringApplication.run(SpringBootMainApplication.class,args); } } 但是SpringBoot推荐的是用Spring配置类代替Spring xml,所以上方@ImportResource注解可以不用 所以要写一个Spring配置类，然后不需要用@ImportResource注解了 新建config包且在里面创建springConfig类 package com.memorygzs.config; import com.memorygzs.service.HelloService; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; /** * @Configuration:指明当前是一个Spring配置类 **/ @Configuration public class MySpringConfig { //此方法的返回值添加到容器中;容器中这个组件默认的id就是方法名 @Bean public HelloService helloService(){ System.out.println(\"配置类@Bean给容器中添加组件了\"); return new HelloService(); } } Bean下方的方法返回的值会添加到IOC容器中 配置文件占位符 利用${}来获取相应的内容 可以获取随机数或者之前配置的值 person.lastName=张三${random.long} person.age=19 person.boss=false person.birth=2020/1/3 person.maps.k1=v1 person.maps.k2=v2 person.lists=${person.lastName}_${random.uuid},b,c person.dog.name=dog_${random.int} person.dog.age=2 SpringBoot对静态资源的映射规则 1）、所有/webjars/,都去classpath:/META-INF/resources/webjars/找资源 webjars：以jar包的方式引入静态资源 webjars官网 以maven坐标方式引入静态资源（例如：jquery）下载到本地通过上面映射路径找资源 2）、“/”访问当前项目的任何资源（静态资源的文件夹） 在以下路径找（可以把静态资源放到以下路径） \"classpath:/META-INF/resources/\", \"classpath:/resources/\", \"classpath:/static/\"：一般放css、js等静态资源, \"classpath:/public/\", \"/\":当前项目的根路径 访问例子：在上面任意路径下有/js/jquery.js 是这样访问的：localhost:8080/js/jquery.js(不带静态资源文件夹名) Thymeleaf模板引擎 引入Thymeleaf 在pom.xml添加以下坐标 org.springframework.boot spring-boot-starter-thymeleaf 一般html文件都放在“classpath:/templates/”下 使用 导入thymeleaf的名称空间 thymeleaf语法 成功 th:text=\"${hello}\" 渲染文本 ​ th:任意html属性; 来替换原生属性的值 ​ 注意：上面的th:text是显示原有字符 th:utext 会解析html标签 表达式 Simple expressions:（表达式语法） 表达式1: ${...}：获取变量值；OGNL； 1）、获取对象的属性、调用方法 2）、使用内置的基本对象： ${session.foo} 3）、内置的一些工具对象： 表达式2: *{...}：选择表达式：和${}在功能上是一样； 补充：配合 th:object=\"${session.user}\" Name: Sebastian. Surname: Pepper. Nationality: Saturn. *{...}中的*代表${session.user}这个对象 表达式3: #{...}：获取国际化内容 表达式4: @{...}：定义URL； @{/order/process(execId=${execId},execType='FAST')} 表达式5: ~{...}：片段引用表达式 ... Literals（字面量） Text literals: 'one text' , 'Another one!' ,… Number literals: 0 , 34 , 3.0 , 12.3 ,… Boolean literals: true , false Null literal: null Literal tokens: one , sometext , main ,… Text operations:（文本操作） String concatenation: + Literal substitutions: |The name is ${name}| Arithmetic operations:（数学运算） Binary operators: + , - , * , / , % Minus sign (unary operator): - Boolean operations:（布尔运算） Binary operators: and , or Boolean negation (unary operator): ! , not Comparisons and equality:（比较运算） Comparators: > , = , th:each的使用 香蕉 10 水果 [[]] 和 [()] [[${user}]] 扩展SpringMVC 使用自己的配置类 在config包中创建MyMvcConfig类（springmvc配置类） package com.memorygzs.config; import org.springframework.context.annotation.Configuration; import org.springframework.web.servlet.config.annotation.ViewControllerRegistry; import org.springframework.web.servlet.config.annotation.WebMvcConfigurer; @Configuration public class MyMvcConfig implements WebMvcConfigurer { @Override public void addViewControllers(ViewControllerRegistry registry) { //将访问路径 /memorygzs 映射到success.html页面 registry.addViewController(\"/memorygzs\").setViewName(\"success\"); } //配置拦截器 @Override public void addInterceptors(InterceptorRegistry registry) { registry.addInterceptor(new LoginHandlerInterceptor()).addPathPatterns(\"/**\").excludePathPatterns(\"/\", \"/login.html\"); //addInterceptor添加配置类，addPathPatterns添加要拦截的路径 /**表示拦截所有，excludePathPatterns添加不要拦截的路径 } } @EnableWebMvc注解全面接管Springmvc（在配置类上添加注解） 相当于重写一个SpringMvc的配置类 springboot的一些Springmvc自动配置失效 这个注解一般不用 因为springboot自动帮你配的一些配置都会失效 比如：静态文件的位置 SpringBoot定制错误的页面 定制错误的页面 将自己制作的html错误页面放到templates下的error文件夹，并且html文件名为http错误状态码 比如：404错误，则html文件名为404.html 可以使用4xx和5xx作为错误页面的文件名来匹配这种类型的所有错误，精确优先 SpringBoot与数据访问 整合JDBC 在pom.xml添加以下坐标 org.springframework.boot spring-boot-starter-data-jdbc mysql mysql-connector-java runtime 在SpringBoot配置文件中添加以下内容（我这里用yml文件） spring: datasource: username: root password: root url: jdbc:mysql://192.168.42.251:3306/db_xkblog?serverTimezone=Asia/Shanghai driver-class-name: com.mysql.cj.jdbc.Driver 整合Druid数据源 在pom.xml添加以下坐标 com.alibaba druid 1.1.23 在SpringBoot配置文件中添加以下内容 spring: datasource: type: com.alibaba.druid.pool.DruidDataSource 和上面一样 更换数据源在spring下datasource的type指定数据源 Druid属性配置 在SpringBoot配置文件中的spring下DataSource下添加以下内容： initialSize: 5 minIdle: 5 maxActive: 20 maxWait: 60000 timeBetweenEvictionRunsMillis: 60000 minEvictableIdleTimeMillis: 300000 validationQuery: SELECT 1 FROM DUAL testWhileIdle: true testOnBorrow: false testOnReturn: false poolPreparedStatements: true #配置监控统计拦截的filters，去掉后监控界面sql无法统计，'wall'用于防火墙 filters: stat,wall,log4j maxPoolPreparedStatementPerConnectionSize: 20 useGlobalDataSourceStat: true connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=500 添加以上内容配置未生效 ,要执行以下操作才会生效（配置Druid类） 在config包下新建DruidConfig类 @Configuration public class DruidConfig { @ConfigurationProperties(prefix = \"spring.datasource\") @Bean public DataSource druid(){ return new DruidDataSource(); } //配置Druid的监控 //1、配置一个管理后台的Servlet @Bean public ServletRegistrationBean statViewServlet(){ ServletRegistrationBean bean = new ServletRegistrationBean(new StatViewServlet(), \"/druid/*\"); Map initParams = new HashMap<>(); initParams.put(\"loginUsername\",\"admin\"); initParams.put(\"loginPassword\",\"123456\"); initParams.put(\"allow\",\"\");//默认就是允许所有访问 bean.setInitParameters(initParams); return bean; } //2、配置一个web监控的filter @Bean public FilterRegistrationBean webStatFilter(){ FilterRegistrationBean bean = new FilterRegistrationBean(); bean.setFilter(new WebStatFilter()); Map initParams = new HashMap<>(); initParams.put(\"exclusions\",\"*.js,*.css,/druid/*\"); bean.setInitParameters(initParams); bean.setUrlPatterns(Arrays.asList(\"/*\")); return bean; } } 浏览器访问：http://localhost:8080/druid 可进入Druid后台监控 账号密码在上面配置类设置 整合Mybatis 在pom.xml添加以下坐标 org.mybatis.spring.boot mybatis-spring-boot-starter 2.1.2 xml配置版 在SpringBoot主配置文件中配置mybatis相关内容以及数据库相关信息 此处用properties和yml都可以，当前用properties,mysql是8.0 #数据库连接信息 spring.datasource.username=root spring.datasource.password=root spring.datasource.url=jdbc:mysql://192.168.1.105:3306/mybatis?serverTimezone=Asia/Shanghai spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver #mybatis相关信息 #config-location指的是全局配置文件在哪个位置 mybatis.config-location=classpath:mybatis/mybatis-config.xml #mapper-locations指的是mapper映射文件在哪个位置 mybatis.mapper-locations=classpath:mybatis/mapper/*.xml 在dao包中新建IStudentDao接口 //此处可以用@Mapper将接口扫描到spring IOC容器中 //或者在SpringBoot主配置类中添加注解@MapperScan来指定dao包路径将接口扫描到spring IOC容器中 @Repository public interface IStudentDao { List findAll(); } 在resources下mybatis文件夹中的mapper新建StudentDaoMapper.xml映射文件 select * from student 在resources下mybatis文件夹中新建mybatis全局配置文件mybatis-config.xml 最后可以通过test测试类来测试mybatis是否整合成功 注意：注解版不需要配置任何内容 按照原来的注解方式来做 SpringBoot缓存 注解版缓存 在SpringBoot主配置文件中添加以下注解 @EnableCaching 缓存注解介绍 | Cache | 缓存接口，定义缓存操作。实现有：RedisCache、EhCacheCache、ConcurrentMapCache等 | | :----------------: | :----------------------------------------------------------- | | CacheManager | 缓存管理器，管理各种缓存（Cache）组件 | | @Cacheable | 主要针对方法配置，能够根据方法的请求参数对其结果进行缓存（一般用于查询数据时使用）属性：cacheNames/value:指定缓存组件的名字 | | @CacheEvict | 主要针对方法配置,清空缓存（一般用于删除数据时使用） | | @CachePut | 主要针对方法配置,保证方法被调用（一定会执行方法），又希望结果被缓存。(修改数据库内容，同时更新缓存，一般用于更新数据时使用) | | @Caching | 主要针对方法配置，用于指定多个缓存规则，以上三个注解的结合。例如:@Caching(cacheable={@Cacheable=()},put={@CachePut=()}） | | @EnableCaching | 开启基于注解的缓存,用于Config文件 | | keyGenerator | 缓存数据时key生成策略 | | serialize | 缓存数据时value序列化策略 | spEL表达式 名字 位置 描述 示例 methodName root object 当前被调用的方法名 #root.methodName method root object 当前被调用的方法 #root.method.name target root object 当前被调用的目标对象 #root.target targetClass root object 当前被调用的目标对象类 #root.targetClass args root object 当前被调用的方法的参数列表 #root.args[0] caches root object 当前方法调用使用的缓存列表如@Cacheable(value={\"cache1\", \"cache2\"})），则有两个cache #root.caches[0].name argument name evaluation context 方法参数的名字. 可以直接 #参数名 ，也可以使用 #p0或#a0 的形式，0代表参数的索引； #id 、 #a0 、 #p0 result evaluation context 方法执行后的返回值（仅当方法执行之后的判断有效，如‘unless’，’cache put’的表达式 ’cache evict’的表达式,'cacheable'中不可用’'beforeInvocation=false） #result @CacheEvict、@Cacheable、@CachePut如果要对同一个实体类对象进行缓存操作，则缓存组件的名字（cacheNames/value）一致，key根据实际情况来定（一般一个实体对象Service中，缓存组件的名字基本上都是相同的） 缓存名字一个一个写太麻烦了，用@CacheConfig注解可以指定这个实体对象Service缓存组件的名字，见下面操作 @CachePut更新缓存流程 首先我们先根据id查询一条数据，此时key为id的值，value为返回的实体类对象，并且存到缓存中 然后再次访问时从缓存中取出数据 再执行更新方法，修改数据库数据，此时更新的数据已经存到了缓存中，但是，它的key和value默认都是返回的实体类对象 更新的数据已经存到了缓存中，但是key的值不一样，所以我们再次查询时是为更新的数据 所以最后我们要在@CachePut属性中指定key的值，例如：#employee.id或者#result.id，根据哪个key来更新缓存 注意：你查询时的缓存中的key要和更新时缓存中的key一致，不然会导致已更新缓存，但是key不一样，查询的数据还是未更新时的数据 @CacheConfig注解介绍 使用了这个注解，下面方法上用其他缓存注解时就不需要指定value或者cacheNames了 在要指定的实体对象Service实现类中加入@CacheConfig注解 @CacheConfig(cacheNames=\"emp\")/*默认是value，value和cacheNames都是指缓存组件名字*/ @Service public class EmployeeServiceImpl implements IEmployeeService { @Autowired IEmployeeDao iEmployeeDao; } "},"MyNotes/Java相关笔记/RestFul风格.html":{"url":"MyNotes/Java相关笔记/RestFul风格.html","title":"ResFul风格","keywords":"","body":"RestFul风格 REST是一种软件架构风格，或者说是一种规范，其强调HTTP应当以资源为中心，并且规范了URI的风格；规范了HTTP请求动作（GET/PUT/POST/DELETE/HEAD/OPTIONS）的使用，具有对应的语义。 核心概念包括： 资源（Resource）： 在REST中，资源可以简单的理解为URI，表示一个网络实体。比如，/users/1/name，对应id=1的用户的属性name。既然资源是URI，就会具有以下特征：名词，代表一个资源；它对应唯一的一个资源，是资源的地址。 表现（Representation）： 是资源呈现出来的形式，比如上述URI返回的HTML或JSON，包括HTTP Header等； REST是一个无状态的架构模式，因为在任何时候都可以由客户端发出请求到服务端，最终返回自己想要的数据，当前请求不会受到上次请求的影响。也就是说，服务端将内部资源发布REST服务，客户端通过URL来定位这些资源并通过HTTP协议来访问它们。 例如： 从上图可以看出请求路径相同但请求方式不同，所代表的业务操作也不同，例如，/advertiser/1这个请求，带有GET、PUT、DELETE三种不同的请求方式，对应三种不同的业务操作。 虽然REST看起来还是很简单的，实际上我们往往需要提供一个REST框架，让其实现前后端分离架构，让开发人员将精力集中在业务上，而并非那些具体的技术细节。实际工作中，一般采用基于SSM（Spring+SpringMVC+Mybatis）框架实现REST风格的软件架构。 https://www.runoob.com/w3cnote/restful-architecture.html "},"MyNotes/Java相关笔记/连接池.html":{"url":"MyNotes/Java相关笔记/连接池.html","title":"连接池","keywords":"","body":"连接池 作用 连接池的作用就是管理连接，因为如果单纯的用JDBC去连接的话，频繁的开启 和关闭连接对于数据库也是一种负担。使用连接池技术后，可以不必频繁开启关闭连接 需要的时候就去用就好了,这样是资源重用，而且有更快的系统反应速度 常用的连接池 常用的连接池的性能分析:https://www.cnblogs.com/linjian/p/4831088.html C3P0(已停止更新) 是一个开放源代码的JDBC连接池，它在lib目录中与Hibernate一起发布,包括了实现jdbc3和jdbc2扩展规范说明的Connection 和Statement 池的DataSources 对象。 Maven坐标 com.mchange c3p0 0.9.5.5 jar compile DBCP(推荐) 下载地址:https://commons.apache.org/proper/commons-dbcp/download_dbcp.cgi DBCP是Apache下独立的数据库连接池组件，在Tomcat中使用的连接池组件就是DBCP，支持JDBC3，JDBC4。 Maven坐标 commons-dbcp commons-dbcp 1.4 druid(阿里出品，推荐) GitHub地址:https://github.com/alibaba/druid 阿里开源的druid不单纯是一个连接池，还添加了监控功能，目前已经是非常受推崇的连接池组件，详细配置参数请参考官网。 当然，还存在一些其他的数据库连接池实现，例如：Tomcat自己就实现了一个连接池组件，根据官方的说法，这个连接池正是为了在Tomcat中替换DBCP，详见：https://tomcat.apache.org/tomcat-7.0-doc/jdbc-pool.html。 Maven坐标 com.alibaba druid 1.1.21 JDBC连接池的使用 DBCP连接池 依赖jar包 commons-dbcp.jar 下载地址：https://downloads.apache.org/commons/dbcp/binaries/ commons-pool.jar 下载地址： https://downloads.apache.org/commons/pool/binaries/ 第一种方法 @Test public void demo1() throws SQLException { // 使用BasicDataSource 创建连接池 BasicDataSource basicDataSource = new BasicDataSource(); // 创建连接池 一次性创建多个连接池 // 连接池 创建连接 ---需要四个参数 basicDataSource.setDriverClassName(\"com.mysql.jdbc.Driver\"); basicDataSource.setUrl(\"jdbc:mysql://localhost:3306/day14\"); basicDataSource.setUsername(\"root\"); basicDataSource.setPassword(\"123\"); // 从连接池中获取连接 Connection conn = basicDataSource.getConnection(); String sql = \"select * from account\"; PreparedStatement stmt = conn.prepareStatement(sql); ResultSet rs = stmt.executeQuery(); while (rs.next()) { System.out.println(rs.getString(\"name\")); } JDBCUtils.release(rs, stmt, conn); } 第二种方法（读取配置文件） dbcp.properties文件： driverClassName=com.mysql.jdbc.Driver url=jdbc:mysql://localhost:3306/day14 username=root password=123 @Test public void demo2() throws Exception { // 读取dbcp.properties ---- Properties Properties properties = new Properties(); properties.load(new demo2().getClass().getResourceAsStream(\"dbcp.properties\")); DataSource basicDataSource = BasicDataSourceFactory .createDataSource(properties); // 从连接池中获取连接 Connection conn = basicDataSource.getConnection(); String sql = \"select * from account\"; PreparedStatement stmt = conn.prepareStatement(sql); ResultSet rs = stmt.executeQuery(); while (rs.next()) { System.out.println(rs.getString(\"name\")); } JDBCUtils.release(rs, stmt, conn); } Druid连接池 依赖包 druid-1.0.9.jar 使用方法和上面的一样，修改下面内容即可 将BasicDataSourceFactory改成DruidDataSourceFactory "},"MyNotes/Java相关笔记/MySql字段类型对应Java实体类型.html":{"url":"MyNotes/Java相关笔记/MySql字段类型对应Java实体类型.html","title":"MySql字段类型对应Java实体类型","keywords":"","body":"MySql字段类型对应Java实体类型 MySql 类型名 GetColumnClassName 返回值 返回的 Java 类 bit(1) (MySQL-5.0) BIT java.lang.Boolean bit(大于1) (MySQL-5.0) BIT byte[] tinyint TINYINT 如果 tinyInt1isBit 配置设置为 true(默认为 true)，是java.lang.Boolean，存储空间为 1；否则是为 java.lang.Integer bool boolean TINYINT 参见 TINYINT。这些是 TINYINT(1) 另一种写法 smallint(M) [unsigned] SMALLINT [UNSIGNED] java.lang.Integer(不管是否无符) mediumint(M) [unsigned] MEDIUMINT [UNSIGNED] java.lang.Integer int integer(M) [unsigned] INTEGER [UNSIGNED] java.lang.Integer；无符的话是 java.lang.Long bigint(M) [unsigned] BIGINT [UNSIGNED] java.lang.Long；无符的话是 java.math.BigInteger float(M,D) FLOAT java.lang.Float double(M,B) DOUBLE java.lang.Double decimal(M,D) DECIMAL java.math.BigDecimal date DATE java.sql.Date datetime DATETIME java.sql.Timestamp timestamp(M) TIMESTAMP java.sql.Timestamp time TIME java.sql.Time year(2/4) YEAR 如果 yearIsDateType 配置设置为 false，返回的对象类型为 java.sql.Short；如果设置为 true(默认为 true)，返回的对象类型是 java.sql.Date，其具体时间是为一月一日零时零分 char(M) CHAR java.lang.String(除非该列字符集设置为 BINARY，那样返回 byte[]) varchar(M) [binary] VARCHAR java.lang.String(除非该列字符集设置为 BINARY，那样返回 byte[]) binary(M) BINARY byte[] varbinary(M) VARBINARY byte[] tinyblob TINYBLOB byte[] tinytext VARCHAR java.lang.String blob BLOB byte[] text VARCHAR java.lang.String mediumblob MEDIUMBLOB byte[] mediumtext VARCHAR java.lang.String longblob LONGBLOB byte[] longtext VARCHAR java.lang.String enum('value1','value2',...) CHAR java.lang.String set('value1','value2',...) CHAR java.lang.String "},"MyNotes/Python相关笔记/列表和元组.html":{"url":"MyNotes/Python相关笔记/列表和元组.html","title":"列表和元组","keywords":"","body":"列表和元组 一.列表方法（[]表示列表） 1.append 在最后添加元素 list1 = ['a','b'] list1.append('c') 2.count 用来统计某个元素在列表中出现几次 word = ['world','asfas','world','aca','world'] word.conunt('world') 3.extend 把另外一个列表的元素添加到当前列表 list1 = ['a','b'] list2 = ['c','d'] list1.extend(list2) #以上代码相当于以下代码： list1+list2 4.index 获取元素在列表中第一个匹配项的下标值 list1 = ['a','b','c','a'] list1.index('a') #这里会得出0，a在列表中的第一个匹配项的下标是0 5.insert 可以在任意位置添加元素 list1 = ['a','b','c','a'] list1.insert(2,'b') 6.pop 删除元素（默认为最后一个） list1 = ['a','b','c','a'] list1.pop(0) 7.sort 排序 list1 = ['a','c','b','a'] list1.sort() # 降序 vowels.sort(reverse=True) 二.元组（tutle） 以（）表示 基本上方法和list一样 "},"MyNotes/Python相关笔记/字典.html":{"url":"MyNotes/Python相关笔记/字典.html","title":"字典","keywords":"","body":"字典 重要方法 1.forkeys() 把list转成字典，并且以list列表中的每一个元素为key组成字典，value值默认为None >>> li = ['name','age'] >>> {}.fromkeys(li) {'name': None, 'age': None} >>> dict.fromkeys(li) {'name': None, 'age': None} 2.pop() 移除元素 >>> dic = {'name':'zhou','age':'18'} >>> dic.pop('name') 'zhou' >>> dic {'age': '18'} 方法名 说明 clear 清除字典中所有的项 copy 返回一个具有相同键-值对的新字典 fromkeys 使用给定的键建立新的字典 get 访问字典项 update 利用一个字典项更新另外一个字典 values 以列表的形式返回列表中的值 "},"MyNotes/Python相关笔记/字符串相关.html":{"url":"MyNotes/Python相关笔记/字符串相关.html","title":"字符串相关","keywords":"","body":"字符串相关知识 #join()连接字符串 seq = ['a','s','fass'] ''.join(seq) #把列表里的全部元素拼成一个字符串，输出'asfass' #strip()去除左右两边空格 str = ' abdasfafa ' str.strip() #输出'abdasfafa' #lower()把所有大写转成小写 str = 'ABC abc' str.lower() #输出'abc abc' #islower() 判断这个字符串是否全为小写 #capitalize() 把这个字符串中的首字符大写，其他全变成小写 str = 'ABC abc' str.capitalize() #输出 'Abc abc' #title() 字符串中所有单词的首字母大写，单词其余部分小写 str = 'ABC abc' str.title() #输出 'Abc Abc' #istitle() 判断字符串中的单词首字母是否大写 #upper() 把所有的小写转成大写 str = 'ABC abc' str.title() #输出 'ABC ABC' #isupper() 字符串是否全为大写 "},"MyNotes/Python相关笔记/条件与循环.html":{"url":"MyNotes/Python相关笔记/条件与循环.html","title":"条件与循环","keywords":"","body":"条件与循环 #三元表达式 for + if 结合 >>> a = [i for i in range(12)] >>> a [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] >>> a = [i for i in range(12) if i > 4] >>> a [5, 6, 7, 8, 9, 10, 11] #for中的else (如果for中的代码未执行或者false的时候，则执行else中的语句) >>> for i in range(12): if i "},"MyNotes/Python相关笔记/正则表达式.html":{"url":"MyNotes/Python相关笔记/正则表达式.html","title":"正则表达式","keywords":"","body":"正则表达式 语法 说明 实例 匹配结果 . 匹配除\\n以外的所有字符 af.c af[a-z]c ^ 匹配字符串的开头 ^123 123fas $ 匹配字符串的末尾 abc$ 456abc [] 匹配[]中的字符 [a-zA-Z0-9] Z [^] 匹配除[]中的所有字符 [^a-zA-Z] 5 * 匹配前面一个字符0次或者多次 r* rrrr + 匹配前面一个字符1次或者多次 ta+ a ? 匹配前面一个字符0次或者1次 ab? b {n} 匹配前面一个字符n次 ab{5} abbbbb { n, m} 匹配前面一个字符n-m次 ho{1,5}b ho \\w 匹配字母数字及下划线 hv\\w hv_ \\s 匹配任意空白字符，等价于 [\\t\\n\\r\\f] qwe\\s que \\d 匹配任意数字，等价于 [0-9] ab\\d ab5 1.match ​ re.match() 用于匹配开始位置的字符串，若开始位置未匹配到则返回NONE 2.search ​ re.search() 只匹配一次对应的值，若后面还有对应的值则不匹配 3.findall ​ re.findall() 在全部的字符串中匹配对应的值 4.sub ​ re.sub() 用正则来表示要被替换的字符串 5.split ​ re.split() 用正则来表示要以什么字符串切割 6.compile ​ re.compile() 使字符串中的正则表达式转换成正则对象 "},"MyNotes/Python相关笔记/类.html":{"url":"MyNotes/Python相关笔记/类.html","title":"类","keywords":"","body":"类（Class） class ab(ccc): #定义class ccc为继承类的名字 if __name__=\"__main__\": #表示每个py程序的入口，执行从这里开始 __name__是当前包的名称，而“__main__”是字符串，它是当前程序运行时包的值，所以当程序运行的时候，__name__就等于__main__ #而当别的程序import导入该class类的时候，当前__name__的值时该类的名字，所有__name__不等于__main__,由此得出，if __name__=\"__main__\"只做于当前程序测试来用 "},"MyNotes/Python相关笔记/Scrapy相关.html":{"url":"MyNotes/Python相关笔记/Scrapy相关.html","title":"Scrapy相关","keywords":"","body":"Scrapy相关 环境搭建 Windows安装 #前提要安装好Python，然后打开cmd，进入到相关的目录，创建Scrapy项目 #安装Scrapy pip install scrapy #创建Scrapy项目 scrapy startproject demo #进入demo文件夹 cd demo #创建一个spider,最后面接域名 scrapy genspider demospider baidu.com #启动spider scrapy crawl demospider Scrapy项目文件介绍 |-demo 项目名 ​ |-init.py 初始化文件 ​ |-items.py 存放items对象，相当于一个容器，和字典较像 ​ |-middlewares.py 定义Downloader Middlewares(下载器中间件) ​ |-pipelines.py 定义Item Pipeline的实现，实现数据的清洗，储存，验证。 ​ |-settings.py 用于设置Scrapy项目爬虫的相关信息（全局配置） ​ |-spiders spider爬虫文件 |-__init__.py **初始化文件** ​ |-demospider.py 名称为demospider.py的Spider爬虫文件 |-scrapy.cfg 配置文件 "},"MyNotes/Python相关笔记/Flask相关.html":{"url":"MyNotes/Python相关笔记/Flask相关.html","title":"Flask相关","keywords":"","body":"Flask相关 Flask-SQLalchemy 蓝本（蓝图）解释 为了在一个或多个应用中，使应用模块化并且支持常用方案， Flask 引入了 蓝图 概念。蓝图可以极大地简化大型应用并为扩展提供集中的注册入口。 Blueprint 对象与 Flask 应用对象的工作方式类似，但不是一 个真正的应用。它更像一个用于构建和扩展应用的 蓝图 。 通俗易懂的解释 当我们写route路由的时候，一般会有很多个路由，甚至分前端路由和后端路由，当发生错误时，如果把这些路由写在一个文件中会很难找到错误的原因，所有这里就要用蓝本。 我们可以创建多个python文件（比如：前端，后端），在每个文件中导入Blueprint（蓝本），然后利用蓝本去写路由，最后通过蓝本把所有的路由进行合并。 有人会提出：直接导包不就行了吗？ 导包可以，但是不能把所有的路由进行合并！ 为什么使用蓝图？ Flask 中蓝图有以下用途： 把一个应用分解为一套蓝图。这是针对大型应用的理想方案：一个项目可以实例化 一个应用，初始化多个扩展，并注册许多蓝图。 在一个应用的 URL 前缀和（或）子域上注册一个蓝图。 URL 前缀和（或）子域的 参数成为蓝图中所有视图的通用视图参数（缺省情况下）。 使用不同的 URL 规则在应用中多次注册蓝图。 通过蓝图提供模板过滤器、静态文件、模板和其他工具。蓝图不必执行应用或视图 函数。 当初始化一个 Flask 扩展时，为以上任意一种用途注册一个蓝图。 蓝图的概念 在蓝图被注册到应用之后，所要执行的操作的集合。当分配请求 时， Flask 会把蓝图和视图函数关联起来，并生成两个端点之前的 URL 。 1.新建Flask可视化项目 项目命名为ViewData， 项目中新建文件夹app/models存放数据库映射类，app/static/js存放js的外部工具包，app/templates/main存放html前端页面，app/templates/errors存放404之后的html前端页面，app/views存放SQLAlchemy的语法的数据库访问类。 2.新建app/init.py 新建flask的初始化类，自定义函数def create_app(config_name)封装一个方法，专门用于创建Flask实例；自定义函数def config_errorhandler(app)编写访问失败之后的404页面的跳转设置。 from flask import Flask, render_template from app.config import config from app.extensions import config_extensions from app.views import config_blueprint # 封装一个方法，专门用于创建Flask实例 def create_app(config_name): # development # 创建应用实例 app = Flask(__name__) # 初始化配置 app.config.from_object(config.get(config_name) or config['default']) # 调用初始化函数 config[config_name].init_app(app) # 配置扩展 config_extensions(app) # 配置蓝本 config_blueprint(app) # 错误页面定制 config_errorhandler(app) # 返回应用实例 return app def config_errorhandler(app): # 如果在蓝本定制，只针对本蓝本的错误有效， # 可以使用app_errorhandler定制全局有效的错误显示 @app.errorhandler(404) def page_not_found(e): return render_template('errors/404.html') 3.新建app/config.py 新建flask连接数据库的配置类，将flask框架连接数据库的配置编写成函数，包括数据库的ip、端口、用户名、密码等。 import os base_dir = os.path.abspath(os.path.dirname(__file__)) # 通用配置 class Config: # 秘钥 SECRET_KEY = os.environ.get('SECRET_KEY') or '123456' # 数据库 SQLALCHEMY_COMMIT_ON_TEARDOWN = True SQLALCHEMY_TRACK_MODIFICATIONS = False # 额外的初始化操作，即使什么内容都没有写，也是有意义的 @staticmethod def init_app(app): pass # 开发环境 语法：mysql+pymysql://用户名：密码@ip：端口/数据库名 class DevelopmentConfig(Config): SQLALCHEMY_DATABASE_URI = 'mysql+pymysql://root:root@localhost:3306/visiondata' # 测试环境 class TestingConfig(Config): SQLALCHEMY_DATABASE_URI = 'mysql+pymysql://root:root@localhost:3306/visiondata' # 生产环境 class ProductionConfig(Config): SQLALCHEMY_DATABASE_URI = 'mysql+pymysql://root:root@localhost:3306/visiondata' # 配置字典 config = { 'development': DevelopmentConfig, 'testing': TestingConfig, 'production': ProductionConfig, 'default': DevelopmentConfig } 4.新建app/ extensions.py 导入flask相关模块，创建SQLAlchemy对象并初始化Bootstrap。自定义函数def config_extensions(app)完成SQLAlchemy和Bootstrap的初始化。 # 导入类库 from flask_bootstrap import Bootstrap from flask_sqlalchemy import SQLAlchemy from flask_migrate import Migrate from flask_moment import Moment # 创建对象 bootstrap = Bootstrap() db = SQLAlchemy() moment = Moment() migrate = Migrate(db=db) # 初始化 def config_extensions(app): bootstrap.init_app(app) db.init_app(app) moment.init_app(app) migrate.init_app(app) 5.新建app/models/表名.py 编写flask的数据库表的映射类，每个表格对应一个文件，映射类命名与表名一致。 表hot_work from app.extensions import db class HotWork(db.Model): __tablename__ = 'hot_work' id = db.Column(db.Integer, primary_key=True) job_name = db.Column(db.String(255)) job_number = db.Column(db.Integer) 表bigdata_work from app.extensions import db class BigDataWork(db.Model): __tablename__ = 'bigdata_work' id = db.Column(db.Integer, primary_key=True) job_name = db.Column(db.String(255)) company_name = db.Column(db.String(255)) city = db.Column(db.String(255)) job_require = db.Column(db.Text) recruit_number = db.Column(db.String(255)) money = db.Column(db.String(255)) skill_require = db.Column(db.String(255)) release_date = db.Column(db.String(255)) sex = db.Column(db.String(255)) company_detail = db.Column(db.String(255)) education = db.Column(db.String(255)) 表avg_money_city from app.extensions import db class AvgMoneyCity(db.Model): __tablename__ = 'avg_money_city' id = db.Column(db.Integer, primary_key=True) city = db.Column(db.String(255)) avg_money = db.Column(db.String(255)) 表avg_money_bigdata from app.extensions import db class AvgMoneyBigData(db.Model): __tablename__ = 'avg_money_bigdata' id = db.Column(db.Integer, primary_key=True) city = db.Column(db.String(255)) avg_money = db.Column(db.String(255)) 6.新建app/models/init.py 编写flask的访问数据库的初始化文件，导入数据库映射表类。 from app.extensions import db from .hot_work import HotWork from .bigdata_work import BigDataWork from .avg_money_city import AvgMoneyCity from .avg_money_bigdata import AvgMoneyBigData 7.新建app/static/js 将html前段需要用到的js工具包存放在js文件夹下 Echarts官网：https://www.echartsjs.com/zh/index.html Echarts镜像网站：https://cdn.bootcss.com/echarts/4.0.4/echarts.min.js （4.0.4是版本号） JQuery官网：https://jquery.com 8.新建ViewData/mamage.py 新建flask启动类，创建flask实例和数据库迁移命令，以及程序的入口。 import os from flask_migrate import MigrateCommand from flask_script import Manager, Server from app import create_app # 获取配置 config_name = os.environ.get('FLASK_CONFIG') or 'default' # 创建Flask实例 app = create_app(config_name) # 创建命令行启动控制对象 manager = Manager(app) manager.add_command(\"runserver\", Server(use_debugger=True)) # 添加数据库迁移命令 manager.add_command('db', MigrateCommand) # 启动项目 if __name__ == '__main__': manager.run() 9.新建app/views/init.py 新建flask的蓝本配置初始化类，进行蓝本的配置，自定义函数def config_blueprint(app)以及封装函数完成蓝本的注册。 from .main import main # 蓝本配置 DEFAULT_BLUEPRINT = ( # 蓝本，前缀 (main, ''), ) # 封装函数，完成蓝本注册 def config_blueprint(app): for blueprint, prefix in DEFAULT_BLUEPRINT: app.register_blueprint(blueprint, url_prefix=prefix) 10.新建app/views的main.py (1)根据数据库表格hotwork，分析并统计招聘数量最多的前十名热门职位,分别使用柱状图表达 数据源：数据库visiondata中hot_work表 在app/views下的main.py添加函数def get_hot_work() ，使用SQLAlchemy语法编写sql的查询语句，查询统计招聘数据前十的热门职位。 # 统计招聘数量最多的前十热门职位 获取结果： def get_hot_work(): # 编写查询语句 返回前10条 # sql: select * from hot_work group by job_name limit 10 ghw_rs = HotWork.query.order_by(desc('job_number')).limit(10) print(ghw_rs) return ghw_rs (2)请根据指定表中的数据，统计出全国某些城市指定招聘岗位平均工资，通过南丁格尔玫瑰图进行呈现。 数据源：数据库visiondata中avg_money_city和avg_money_bigdata 在app/views下的main.py添加函数def get_avg_money_city()和def get_avg_money_BigData()，使用SQLAlchemy语法编写sql的查询语句，查询两个表中的城市平均工资数据。 # 所有城市招聘数据的平均工资 获取结果 def get_avg_money_city(): # sql: select * from avg_money_city gamc_rs = AvgMoneyCity.query.all() print('所有城市招聘数据的平均工资 获取结果成功') return gamc_rs # “大数据”相关职位所有城市招聘数据的平均工资 获取结果 def get_avg_money_BigData(): # sql: select * from avg_money_big_data gamb_rs = AvgMoneyBigData.query.all() print('“大数据”相关职位所有城市招聘数据的平均工资 获取结果成功') return gamb_rs (3)分析并统计招聘数量\"大数据\"相关职位招聘数量,分别使用柱状图表达，同时在网页后台输出相关数据打印语句。 数据源：数据库visiondata中bigdata_work 在app/views下的main.py添加函数def get_big_data_work()，使用SQLAlchemy语法编写sql的查询语句，对相同职位进行数量汇总。 # 对相同职位进行数量汇总，\"大数据\"相关职位招聘数量比较 def get_big_data_work(): # sql: select job_name,count(*) from bigdata_work where job_name like '%大数据%' GROUP BY job_name gbdw_rs = db.session.query(BigDataWork.job_name, func.count('*').label('job_count'))\\ .group_by(BigDataWork.job_name).order_by(desc('job_count')).all() print('\"大数据\"相关职位招聘数量：' + str(gbdw_rs)) return gbdw_rs app/views下的main.py完整代码，如下所示： from app.extensions import db from app.models import HotWork, BigDataWork, AvgMoneyCity, AvgMoneyBigData from flask import Blueprint, render_template from sqlalchemy import * main = Blueprint('main', __name__) # 实例化路由 @main.route('/') def index(): return render_template('/main/echarts.html') # 当url访问 / 直接跳转到主页 @main.route('/index/') # url访问 /index/ 跳转主页 调用display方法 将查询结果通过render_template()传入html页面 def display(): rs_ghw = get_hot_work() rs_gbdw = get_big_data_work() rs_gamc = get_avg_money_city() rs_gamb = get_avg_money_BigData() return render_template('/main/echarts.html', rs_ghw=rs_ghw, rs_gbdw=rs_gbdw, rs_gamc=rs_gamc, rs_gamb=rs_gamb) # 统计招聘数量最多的前十热门职位 获取结果： def get_hot_work(): # 编写查询语句 返回前10条 # sql: select * from hot_work group by job_name limit 10 ghw_rs = HotWork.query.order_by(desc('job_number')).limit(10) print(ghw_rs) return ghw_rs # 所有城市招聘数据的平均工资 获取结果 def get_avg_money_city(): # sql: select * from avg_money_city gamc_rs = AvgMoneyCity.query.all() print('所有城市招聘数据的平均工资 获取结果成功') return gamc_rs # “大数据”相关职位所有城市招聘数据的平均工资 获取结果 def get_avg_money_BigData(): # sql: select * from avg_money_big_data gamb_rs = AvgMoneyBigData.query.all() print('“大数据”相关职位所有城市招聘数据的平均工资 获取结果成功') return gamb_rs # 对相同职位进行数量汇总，\"大数据\"相关职位招聘数量比较 def get_big_data_work(): # sql: select job_name,count(*) from bigdata_work where job_name like '%大数据%' GROUP BY job_name gbdw_rs = db.session.query(BigDataWork.job_name, func.count('*').label('job_count'))\\ .group_by(BigDataWork.job_name).order_by(desc('job_count')).all() print('\"大数据\"相关职位招聘数量：' + str(gbdw_rs)) return gbdw_rs 11.新建app/templates/errors/404.html 编写url访问失败之后的提示页面。 Title 404 12.新建app/templates/main/echarts.html 编写js获取到flask传过来的数据，通过echarts组件进行结果的展示（包括css样式）。 招聘信息统计 html , body , .content { width:100%; height:100%; padding: 0; margin: 0; box-sizing: border-box; background-color: #ccc; } .content { padding: 40px; } .header { height: 10%; width: 100%; font-size: 24px; font-weight: 700; line-height: 60px; text-align: center; } .body { height: 100%; width: 100%; text-align: center; } .chartBox { width: 100%; height: 60%; margin-bottom:40px; } //折线图 招聘数量最多的前十热门职位 var hotWork = echarts.init(document.getElementById('hotWork')); //获取div的id 实例化echarts组件 var data_name = [] //将职位名job_name存放在一个数组中作为x轴数据 var data_y = [] //将职位数量job_number存放在一个数组中作为y轴数据 console.log('十大热门职位:' + data_name); console.log('数据分别为:' + data_y); console.log('最大值: ' + Math.max.apply(null, data_y) + '， 最小值:' + Math.min.apply(null, data_y)); workOption = { title: { text: '职位分析', //主标题 subtext: ' ---10大热门职位分析', //副标题 x: '45%' //设置标题位置 }, xAxis: { type: 'category', name: '岗位名称', data: data_name, axisLabel : { interval: 0, rotate: \"25\" //x轴字体的旋转度 } }, yAxis: { name: '招聘数量', type: 'value' }, series: [{ data: data_y, type: 'line', //设置图形为折线图 label: { normal: { show: true, position: 'top' //折线图顶部显示对应的x轴数值 } } }] }; hotWork.setOption(workOption); //设置echarts的option参数 加载并显示图形 //南丁格尔玫瑰图 var pieAvgMoneyCity = echarts.init(document.getElementById('pieAvgMoneyCity')); var city = [] var avg_money_pie_city = [] var avg_money_pie_bigdata = [] var avg_money_city = [] var avg_money_big_data = [] var dataInt_city = []; var dataInt_big_data = []; dataInt_city.forEach(function(data){ avg_money_city.push(+parseInt(data)); ///遍历数组 将每个元素变成整型 }); dataInt_big_data.forEach(function(data){ avg_money_big_data.push(+parseInt(data)); ///遍历数组 将每个元素变成整型 }); console.log('所有城市平均薪资:' + city); console.log('数据分别为:' + avg_money_city); console.log(\"所有城市最大平均薪资为\" + Math.max.apply(null, avg_money_city)); console.log(\"“大数据”相关职位城市招聘数据的平均工资\" + city); console.log('数据分别为:' + avg_money_big_data); console.log(\"“大数据”相关职位最大平均薪资为\" + Math.max.apply(null, avg_money_big_data)); pieAvgMoneyCityOption = { title : { text: '所有城市招聘数据的平均工资 vs “大数据”相关职位所有城市招聘数据的平均工资', subtext: '南丁格尔玫瑰图', x:'center' }, tooltip : { trigger: 'item', formatter: \"{b}:{c}({d}%)\" //当鼠标移动到图形 显示数据(格式): 佛山（1111） 10% }, legend: { x : 'center', y : 'bottom', data:city }, color:[ '#C1232B','#B5C334','#FCCE10','#E87C25','#27727B', '#668ffe','#00ca54','#00dbfa','#f3006a','#60C0DD', '#d714b7','#84433c','#f490f3','#000000','#26C0C0'], series : [ { type:'pie', radius : [20, 110], //图像的大小 center : ['25%', '50%'], //图形的位置 roseType : 'radius', //南丁格尔玫瑰图的参数 data:avg_money_pie_city }, { type:'pie', radius : [30, 110], center : ['75%', '50%'], roseType : 'area', data:avg_money_pie_bigdata } ] }; pieAvgMoneyCity.setOption(pieAvgMoneyCityOption); //柱状图 \"大数据\"相关职位招聘数量 var bigDataWork = echarts.init(document.getElementById('bigDataWork')); var job_name = [] var quantity = [] // 获取元组中的第二个元素('AI大数据工程师', 1095) console.log('招聘职位: ' + job_name); console.log('对应职位招聘数量: ' + quantity); console.log('岗位需求量最大: ' + Math.max.apply(null, quantity) + '， 岗位需求量最少：' + Math.min.apply(null, quantity)); bigdataworkOption = { title: { text: '大数据相关职位招聘数量', subtext: ' ----职位招聘对比', x: '45%', //modified 0523 textStyle:{ //文字颜色 color:'#f30008', //字体风格,'normal','italic','oblique' fontStyle:'oblique', //字体粗细 'normal','bold','bolder','lighter',100 | 200 | 300 | 400... fontWeight:'bold', //字体系列 fontFamily:'FangSong' //fontFamily: 'KaiTi' //字体大小 //fontSize:18 } //modified end }, xAxis: { type: 'category', name: '职位名称', data: job_name, axisLabel : { interval: 0, rotate: \"15\" //x轴字体的旋转度 } }, yAxis: { type: 'value', name: '招聘数量' }, series: [{ data: quantity, type: 'bar', label: { normal: { show: true, position: 'top' } }, //modified0523 itemStyle: { normal: {color: 'black'} } //modified end }] }; bigDataWork.setOption(bigdataworkOption); 13.运行Flask程序查看结果 （1）分析并统计招聘数量最多的前十名热门职位,分别使用折线图表达 （2）分析并统计招聘数量\"大数据\"相关职位招聘数量,分别使用柱状图表达。 （3）统计出全国某些城市指定招聘岗位平均工资，通过南丁格尔玫瑰图进行呈现。 语法 db.create_all() ：创建实体表 db.drop_all(): 删除表 1）插入表 Db.session.add(user) #user是实体对象 ##批量插入 Db.session.add_all([user1，user2，user3,…..]) ##提交 Db.session.commit() 2) 修改表 如果实体有id，那么就会变成更新操作，如果没有就是默认插入操作 User.name=’xiaobing’ Db.session.add(user) Db.session.commit() 3)删除行 Db.session.delete(user) Db.session.commit() 2,查询 1）filter_by,filter User.query.filter_by(role=user_role).all() user_role = Role.query.filter_by(name='User').first() filter_by是直接根据实体对象去转化sql,filter是比较直接插入到sql中 常用的SQLALchemy查询过滤器 过滤器 说明: filter() 把过滤器添加到原查询上 filter_by() 把等值过滤器添加到原查询上 limit() 限制原查询返回的结果数量 offset() 偏移原始查询返回的结果 order_by() 根据指定条件对原查询进行排序 group_by() 根据指定条件对原查询结果进行分组 在users = db.relationship('User', backref='role', lazy='dynamic') 加入了lazy=’dynamic’参数，从而禁止自动执行查询，user_role.users会返回一个尚未执行的查询，因此可以在其上添加过滤器 user_role.users.order_by(User.username).all() 最常用的SQLALCHEMY列选项 选项名 说明 primary_key 主键 unique 是否允许重复值 index 设为True，为这一 列建立索引 nullable 设为True，允许使用空 default 默认值 2）like 模糊匹配 例如： categorys = Category.query.filter(Category.name.like(\"%\"+keyword+\"%\")).all() 这种好像也可以 hosts.query.whoosh_search('ce').all() 3）连表查询 class Topic(db.Models): __tablename__ = 'topic' id = db.Column(db.Integer, primary_key = True) content = db.Column(db.Text) class Reply(db.Models): __tablename = 'reply' id = db.Column(db.Integer, primary_key = True) topic_id = db.Column(db.Integer) content = db.Column(db.Text) 查询： result=Reply.query.join(Topic, Reply.topic_id==Topic.id).add_entity(Topic).all() 然后遍历值： for res in result： res.Reply.xxx res.Topic.xxx 4）查询返回指定的字段 User.query(User.name,User.email,User.sex).order(User.name.des()).limit(20).skip(200) 5)group by 分组查询 第一种方式： rs =User.query(StudentInfo.cls, func.sum(StudentInfo.cert_count)).group_by(StudentInfo.cls).all() 第二种方式： Create_engine=SQLAlchemy.get_engine() sql = 'select cls, sum(cert_count) from stuinfo group by cls' rs = engine.execute(sql) for row in rs: print row[0], row[1] 6) 分页显示paginate pagination = Post.query.order_by(Post.timestamp.desc()).paginate(page,per_page=current_app.config['ARTISAN_POSTS_PER_PAGE'],error_out=False) 返回的pagination对象 包含以下： Page：当前页 Page_per:每页显示多少条 Total:总条数 "},"MyNotes/大数据组件内容/Hadoop相关.html":{"url":"MyNotes/大数据组件内容/Hadoop相关.html","title":"Hadoop相关","keywords":"","body":"Hadoop相关 Hadoop介绍 Hadoop生态镜像网站：http://archive.apache.org/dist Hadoop HDFS 海量储存 MapReduce 海量计算 YARN ResouceManager 管理和调度 NodeManager 执行任务 ApplicationMaster 向RM申请任务 HDFS NameNode 管理和分配 SecondaryNameNode 辅助NameNode（不是副本） DataNode 用来存储管理 MapReduce Map 拆分 Reduce 合成 Hadoop搭建 一.单机伪分布搭建 配置JDK环境 方法一RPM安装： rpm -ivh --relocate /usr/java=/usr/local/jdk1.8 jdk-8u101-linux-x64.rpm #--relocate指定位置，等号左边路径表示你要安装的路径以及文件名，等号右边指定的是安装包路径 rpm -ivh jdk1.8 jdk-8u101-linux-x64.rpm #直接进入要安装的路径，并且把rpm包移动到安装路径上一级，mv jdk1.8 /usr/local/src/java 方法二安装Tar解压： tar -zxvf jdk-8u231-linux-x64.tar.gz -C /usr/local/src #解压到/usr/local/src目录中 mv /usr/local/src/jdk-8u231-linux-x64 /usr/local/src/jdk SSH免密钥 方法一： ssh-keygen -t rsa -P '' -f 〜/ .ssh/id_rsa 创建id_rsa公钥和私钥 cat ~/.ssh/id_rsa.public >> ~/.ssh/authorized_keys 把id_rsa公钥放到authorized_keys认证文件里面 方法二： ssh-keygen ssh-copy-id 192.168.1.120 配置Hadoop环境 Hadoop清华镜像网: http://mirror.bit.edu.cn/apache/hadoop/common/ tar -zxvf hadoop-2.7.7.tar.gz -C /usr/local/src (解压到/usr/local/src目录中) mv /usr/local/src/hadoop-2.7.7 /usr/local/src/hadoop #配置系统环境变量，若需要配置用户环境变量，则修改 ./bashrc vim /etc/profile 添加： export PATH=$PATH:/usr/local/src/hadoop/bin:/usr/local/src/hadoop/sbin:/usr/local/src/jdk/bin source /etc/profile #（或者.bashrc,按情况来选择） 让环境变量生效 配置Hadoop文件 编辑hadoop-env.sh (修改配置文件中环境变量) #在文件中找到JAVA_HOME 并且指定jdk绝对路径 若没有JAVA_HOME ,则添加export JAVA_HOME=/usr/local/src/jdk #(在文件中配置java环境，这个文件是修改在Hadoop管理脚本远程连接其他服务器中，让java命令有效) #ps：Hadoop管理脚本远程连接其他服务器时是找不到另外一台的java环境，只能在hadoop脚本中添加java环境，就可以使用java命令了 编辑core-site.xml（2.6.0版本，每个版本具体怎么配看官方文档）https://hadoop.apache.org/docs/r2.6.0 （r后面接版本号) #添加（configuration文件中本来就有，只要复制标签内的就行）： fs.defaultFS #默认为namenode,配置namenode hdfs://node01:9000 #如果是完全分布式的话，这里必须设置成主节点的主机名 hadoop.tmp.dir #配置持久化目录 /var/hadoopData/local #hadoopData是空的，随后格式化会自动生成 #（local名字根据搭建什么分布式类型来取，单分布式和完全分布式） 编辑hdfs-site.xml #添加（configuration文件中本来就有，只要复制标签内的就行）： dfs.replication 1 #副本数 dfs.namenode.secondary.http-address node01:50090 --> #只在主节点配置，其他节点可配可不配 dfs.http.address #避免用浏览器访问时发生错误 node01:50070 #node01为主机名 编辑slaves (3.0版本之后改名为workers,设置从服务器的主机名) #因为这里是单机伪分布，所以只需要写上本机的主机名就行了，如果是完全分布式，则添加从服务器 localhost 启动 hdfs namenode -format #格式化namenode start-dfs.sh #启动hdfs 常见错误 ERROR: Attempting to operate on hdfs datanode as root ERROR: but there is no HDFS_DATANODE_USER defined. Aborting operation. 说明没有为各个角色（进程）设置用户，设成root 编辑 /sbin/start-dfs.sh 和 stop-dfs.sh 在最上面添加： HDFS_DATANODE_USER=root HDFS_NAMENODE_USER=root HDFS_SECONDARYNAMENODE_USER=root 若出现浏览器访问 主机名:50070 不成功 则是没有配置dfs.http.address 在hdfs-site.xml添加: dfs.http.address node01:50070 node01为主机名 二.完全分布式搭建 假设有3台服务器，1台主，2台从：node01(主192.168.1.108) node02(从192.168.1.110) node03(从192.168.1.112) 配置node01 Hadoop和JAVA环境和文件（具体看单节点分布式） 修改/etc/hosts文件 192.168.1.108 node01 192.168.1.110 node02 192.168.1.112 node03 修改slaves node02 node03 配置免密钥 复制文件到各个节点 scp -r /usr/local/hadoop node02: /usr/local/hadoop scp -r /usr/src/jdk node02: /usr/src/jdk scp -r /etc/profile node02: /etc/profile scp -r /usr/local/hadoop node03: /usr/local/hadoop scp -r /usr/src/jdk node03: /usr/src/jdk scp -r /usr/src/profile node03: /etc/profile Hadoop HA搭建 Zookeeper搭建 解压Zookeeper （tar包）并且配置环境变量 tar -zxvf zookeeper-3.4.5.tar.gz -C /usr/local/src cd /usr/local/src mv zookeeper-3.4.5 zookeeper vim /etc/profile export ZOOKEEPER_HOME=/usr/local/src/zookeeper #追加在PATH后面 export PATH=$PATH:$ZOOKEEPER_HOME/bin 进入到Zookeeper目录下的conf目录 /usr/local/src/zookeeper/conf cd /usr/local/src/zookeeper/conf #复制配置文件 cp zoo_sample.cfg zoo.cfg vim zoo.cfg dataDir=/usr/local/src/zookeeper/data #日志的存放目录，手动创建 #看你有几台zookeeper服务器 server.0=node01:2888:3888 server.1=node02:2888:3888 server.2=node03:2888:3888 创建日志的存放目录和myid文件 mkdir /usr/local/src/zookeeper/data touch /usr/local/src/zookeeper/data/myid echo 0 > /usr/local/src/zookeeper/data/myid #向myid文件里面写入0，这里要看配置文件对应的服务id 配置hosts文件 复制文件给其他主机 scp -r /usr/local/src/ node02:/usr/local/src scp -r /usr/local/src/ node03:/usr/local/src #传过去之后需要修改myid文件里面的id，而且需要把环境变量文件一起传过去 启动 #三台都启动zookeeper zkServer.sh start 配置HA、NameNode、JournalNode等服务 在单节点配置之上进行修改 编辑hadoop-env.sh 修改hdfs-site.xml文件 （vim hdfs-site.xml） cd /usr/local/src/hadoop/etc/hadoop vim hdfs-site.xml dfs.replication 1 dfs.nameservices mycluster dfs.ha.namenodes.mycluster namenode01,namenode02 dfs.namenode.rpc-address.mycluster.namenode01 node01:8020 dfs.namenode.rpc-address.mycluster.namenode02 node02:8020 dfs.namenode.http-address.mycluster.namenode01 node01:50070 dfs.namenode.http-address.mycluster.namenode02 node02:50070 dfs.namenode.shared.edits.dir qjournal://node01:8485;node02:8485/mycluster dfs.journalnode.edits.dir /var/hadoopData/ha/jn dfs.client.failover.proxy.provider.mycluster org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider dfs.ha.fencing.methods sshfence shell(/bin/true) dfs.ha.fencing.ssh.private-key-files /root/.ssh/id_rsa dfs.ha.automatic-failover.enabled true 修改core-site.xml文件 （vim core-site.xml） fs.defaultFS hdfs://mycluster hadoop.tmp.dir /var/hadoopData/ha ha.zookeeper.quorum node02:2181,node03:2181 编辑slaves（workers） 配置从节点 (vim slaves) node02 node03 两个namenode(node01,node02)之间设置ssh免密钥 node01: cd ~/.ssh/ ssh-keygen ssh-copy-id node02 node02: cd ~/.ssh/ ssh-keygen ssh-copy-id node01 部署 启动所有主机的JournalNode（node01,node02） hadoop-daemon.sh start journalnode 格式化并且启动第一台Namenode 【node01】（初始化，在这之前必须启动JournalNode） hdfs namenode -format hadoop-daemon.sh start namenode 第二台Namenode同步第一台Namenode（要在第一台格式完启动） hdfs namenode -bootstrapStandby 格式化ZKFC并且启动所有服务（第一台NameNode上【node01】） hdfs zkfc -formatZK start-dfs.sh 查看 namenode 状态 hdfs haadmin -getServiceState nn1 hdfs haadmin -getServiceState nn2 查看集群状态 如果 Live datanode 不为 0，则说明集群启动成功 hdfs dfsadmin -report Hadoop使用 mkdir input echo \"hello world\">./input/test1.txt echo \"hello hadoop\">./input/test2.txt hdfs dfs -mkdir /in hdfs dfs -put input/ /in hadoop jar /usr/local/src/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar wordcount /in/input /out hdfs dfs -cat /out/* Yarn搭建（基于Hadoop） 因Yarn是在Hadoop里面的，所有先要配置Hadoop环境 Yarn单节点配置（基本配置，必配） 编辑mapred-site.xml （vim /usr/local/src/hadoop/etc/hadoop/mapred-site.xml） mapreduce.framework.name yarn mapreduce.application.classpath $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/* 编辑yarn-site.xml文件 （vim yarn-site.xml） yarn.nodemanager.aux-services mapreduce_shuffle Yarn HA配置 在单节点的基础上配置 修改yarn-site.xml yarn.resourcemanager.ha.enabled true hadoop.zk.address zk1:2181,zk2:2181,zk3:2181 yarn.resourcemanager.cluster-id cluster1 yarn.resourcemanager.ha.rm-ids rm1,rm2 yarn.resourcemanager.hostname.rm1 master1 yarn.resourcemanager.hostname.rm2 master2 yarn.resourcemanager.webapp.address.rm1 master1:8088 yarn.resourcemanager.webapp.address.rm2 master2:8088 启动 （第一台NameNode） start-yarn.sh Yarn计算测试 计算hdfs中text.txt文件所有的单词数量（wordcount功能） /data/wc/output 输出目录 hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar wordcount /user/root/text.txt /data/wc/output "},"MyNotes/大数据组件内容/Spark相关.html":{"url":"MyNotes/大数据组件内容/Spark相关.html","title":"Spark相关","keywords":"","body":"Spark相关 一.搭建 解压和配置环境变量 wget http://archive.apache.org/dist/spark/spark-2.0.2/spark-2.0.2-bin-hadoop2.6.tgz tar -zxvf spark-2.0.2-bin-hadoop2.6.tgz -C /usr/local/src cd /usr/local/src mv spark-2.0.2-bin-hadoop2.6.tgz spark vim /etc/profile 添加SPARK_HOME 配置 cd /usr/local/src/spark/conf/ #复制以及修改 cp spark-env.sh.template spark-env.sh vim spark-env.sh 添加以下内容： export JAVA_HOME=/usr/local/src/jdk export SPARK_MASTER_HOST=node01 //IP或域名都可以 #配置Zookeeper export SPARK_DAEMON_JAVA_OPTS=\"-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node01:2181,node02:2181,node03:2181 -Dspark.deploy.zookeeper.dir=/spark\" cp slaves.template slaves vim slaves 配置从节点的服务 添加以下内容： node02 node03 进入Spark启动文件,修改启动和停止的脚本，因为名字和hadoop启动脚本文件冲突 cd /usr/local/src/spark/sbin mv ./start-all.sh ./start-spark-all.sh mv ./stop-all.sh ./stop-spark-all.sh mv ./start-master.sh ./start-spark-master.sh mv ./stop-master.sh ./stop-spark-master.sh 启动 start-spark-all.sh [root@node01 conf]# jps 11283 Worker 从节点服务 21695 Master 主节点服务 测试 用浏览器访问node01:8080 若想用多个主节点，也就是开启HA服务 则在第二台备用节点启动Master服务 vim /usr/local/src/spark/conf/spark-env.sh 修改Master地址 start-spark-master.sh 若想要测试HA的可用性，则关闭或杀死Active的Master主节点服务，查看备用节点的8080端口是否变成Active "},"MyNotes/大数据组件内容/Spark RDD.html":{"url":"MyNotes/大数据组件内容/Spark RDD.html","title":"Spark RDD相关","keywords":"","body":"RDD Spark 的核心是建立在统一的抽象弹性分布式数据集（Resiliennt Distributed Datasets，RDD）之上的，这使得 Spark 的各个组件可以无缝地进行集成，能够在同一个应用程序中完成大数据处理。本节将对 RDD 的基本概念及与 RDD 相关的概念做基本介绍。 一.RDD 的基本概念 RDD 是 Spark 提供的最重要的抽象概念，它是一种有容错机制的特殊数据集合，可以分布在集群的结点上，以函数式操作集合的方式进行各种并行操作。 通俗点来讲，可以将 RDD 理解为一个分布式对象集合，本质上是一个只读的分区记录集合。每个 RDD 可以分成多个分区，每个分区就是一个数据集片段。一个 RDD 的不同分区可以保存到集群中的不同结点上，从而可以在集群中的不同结点上进行并行计算。 ​ 图 1 展示了 RDD 的分区及分区与工作结点（Worker Node）的分布关系。 RDD 具有以下几个属性: 只读：不能修改，只能通过转换操作生成新的 RDD。 分布式：可以分布在多台机器上进行并行处理。 弹性：计算过程中内存不够时它会和磁盘进行数据交换。 基于内存：可以全部或部分缓存在内存中，在多次计算间重用。 RDD 实质上是一种更为通用的迭代并行计算框架，用户可以显示控制计算的中间结果，然后将其自由运用于之后的计算 二.RDD 基本操作 RDD 的操作分为转化（Transformation）操作和行动（Action）操作。转化操作就是从一个 RDD 产生一个新的 RDD，而行动操作就是进行实际的计算。 RDD 的操作是惰性的，当 RDD 执行转化操作的时候，实际计算并没有被执行，只有当 RDD 执行行动操作时才会促发计算任务提交，从而执行相应的计算操作。 1.构建操作 Spark 里的计算都是通过操作 RDD 完成的，学习 RDD 的第一个问题就是如何构建 RDD，构建 RDD 的方式从数据来源角度分为以下两类。 从内存里直接读取数据。 从文件系统里读取数据，文件系统的种类很多，常见的就是 HDFS 及本地文件系统。 2.转换操作 RDD 的转换操作是返回新的 RDD 的操作。转换出来的 RDD 是惰性求值的，只有在行动操作中用到这些 RDD 时才会被计算。 许多转换操作都是针对各个元素的，也就是说，这些转换操作每次只会操作 RDD 中的一个元素，不过并不是所有的转换操作都是这样的。表 1 描述了常用的 RDD 转换操作。 ​ 表 1 RDD转换操作（rdd1={1, 2, 3, 3}，rdd2={3,4,5}) 函数名 作用 示例 结果 map() 将函数应用于 RDD 的每个元素，返回值是新的 RDD rdd1.map(x=>x+l) {2,3,4,4} flatMap() 将函数应用于 RDD 的每个元素，将元素数据进行拆分，变成迭代器，返回值是新的 RDD rdd1.flatMap(x=>x.to(3)) {1,2,3,2,3,3,3} filter() 函数会过滤掉不符合条件的元素，返回值是新的 RDD rdd1.filter(x=>x!=1) {2,3,3} distinct() 将 RDD 里的元素进行去重操作 rdd1.distinct() (1,2,3) union() 生成包含两个 RDD 所有元素的新的 RDD rdd1.union(rdd2) {1,2,3,3,3,4,5} intersection() 求出两个 RDD 的共同元素 rdd1.intersection(rdd2) {3} subtract() 将原 RDD 里和参数 RDD 里相同的元素去掉 rdd1.subtract(rdd2) {1,2} cartesian() 求两个 RDD 的笛卡儿积 rdd1.cartesian(rdd2) {(1,3),(1,4)……(3,5)} 3.行动操作 行动操作用于执行计算并按指定的方式输出结果。行动操作接受 RDD，但是返回非 RDD，即输出一个值或者结果。在 RDD 执行过程中，真正的计算发生在行动操作。表 2 描述了常用的 RDD 行动操作。 ​ 表 2 RDD 行动操作（rdd={1,2,3,3}） 函数名 作用 示例 结果 collect() 返回 RDD 的所有元素 rdd.collect() {1,2,3,3} count() RDD 里元素的个数 rdd.count() 4 countByValue() 各元素在 RDD 中的出现次数 rdd.countByValue() {(1,1),(2,1),(3,2})} take(num) 从 RDD 中返回 num 个元素 rdd.take(2) {1,2} top(num) 从 RDD 中，按照默认（降序）或者指定的排序返回最前面的 num 个元素 rdd.top(2) {3,3} reduce() 并行整合所有 RDD 数据，如求和操作 rdd.reduce((x,y)=>x+y) 9 fold(zero)(func) 和 reduce() 功能一样，但需要提供初始值 rdd.fold(0)((x,y)=>x+y) 9 foreach(func) 对 RDD 的每个元素都使用特定函数 rdd1.foreach(x=>printIn(x)) 打印每一个元素 saveAsTextFile(path) 将数据集的元素，以文本的形式保存到文件系统中 rdd1.saveAsTextFile(file://home/test) saveAsSequenceFile(path) 将数据集的元素，以顺序文件格式保存到指 定的目录下 saveAsSequenceFile(hdfs://home/test) "},"MyNotes/大数据组件内容/Hive相关.html":{"url":"MyNotes/大数据组件内容/Hive相关.html","title":"Hive相关","keywords":"","body":"Hive相关 一、hive相关资料 http://blog.csdn.net/u013310025/article/details/70306421 https://www.cnblogs.com/guanhao/p/5641675.html http://blog.csdn.net/wisgood/article/details/40560799 http://blog.csdn.net/seven_zhao/article/details/46520229 二、单用户配置 安装Mysql yum -y install mariadb-server systemctl start mariadb mysqladmin -u root password root mysql -u root -p root 解压Hive包到具体位置 tar -zxvf apache-hive-1.1.0-bin.tar.gz -C /usr/local/src/hive cd /usr/local/src/hive 从MySQL 8开始,您不再可以(隐式)使用GRANT命令创建用户.请改用CREATE USER,然后使用GRANT声明： mysql> CREATE USER 'root'@'%' IDENTIFIED BY 'root'; mysql> GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' WITH GRANT OPTION; 配置环境变量 (vim /etc/profile)** export HIVE_HOME=/usr/local/src/hive 在export PATH=$PATH:后面添加$HIVE_HOME/bin source /etc/profile 复制配置文件 cp hive-env.sh.template hive-env.sh cp hive-default.xml.template hive-site.xml # 创建文件夹，hive-site.xml 中配置需要 mkdir -p /usr/local/src/hive/tmp 编辑hive-env.sh文件 export JAVA_HOME=/usr/local/src/jdk export HADOOP_HOME=/usr/local/src/hadoop export HIVE_HOME=/usr/local/src/hive export HIVE_CONF_DIR=/usr/local/src/hive/conf 编辑hive-site.xml文件 数据库密码：root 没有的 property 则添加，有的则根据需求修改 或者把原有的全部删除：vim中 .,$-1d删除，添加以下内容 system:java.io.tmpdir /usr/local/src/hive/tmp javax.jdo.option.ConnectionURL jdbc:mysql://node01:3306/hive?createDatabaseIfNotExist=true javax.jdo.option.ConnectionDriverName com.mysql.jdbc.Driver javax.jdo.option.ConnectionUserName root javax.jdo.option.ConnectionPassword root sed -i \"s#\\${system:java.io.tmpdir}/\\${system:user.name}#/usr/local/src/hive/iotmp#g\" /usr/local/src/hive/conf/hive-site.xml 复制mysql驱动包到指定文件路径 cp mysql-connector-java-5.1.46-bin.jar /usr/local/src/hive/lib/ 在 hdfs 中创建下面的目录 ，并赋予所有权限 hdfs dfs -mkdir -p /usr/local/src/hive/warehouse hdfs dfs -mkdir -p /usr/local/src/hive/tmp hdfs dfs -mkdir -p /usr/local/src/hive/log hdfs dfs -chmod -R 777 /usr/local/src/hive/warehouse hdfs dfs -chmod -R 777 /usr/local/src/hive/tmp hdfs dfs -chmod -R 777 /usr/local/src/hive/log 初始化 hive schematool -dbType mysql -initSchema #出现以下内容就证明初始化成功 Metastore connection URL: jdbc:mysql://hp:3306/hive?createDatabaseIfNotExist=true Metastore Connection Driver : com.mysql.jdbc.Driver Metastore connection User: root Starting metastore schema initialization to 1.1.0 Initialization script hive-schema-1.1.0.mysql.sql Initialization script completed schemaTool completed 安装hive到此结束，进入hive命令行 hive 常见问题 常见问题 com.google.common.base.Preconditions.checkArgument 原因：这是因为hive内依赖的guava.jar和 hadoop内的版本不一致造成的。 解决方法：查看/share/hadoop/common/lib内guava.jar版本是否和hive目录下lib内guava.jar一致，若 hive目录下lib内版本不对，则将Hadoop中的复制到hive中 因为版本原因，需要重新拷贝一个 jar 包 rm -rf /usr/local/src/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar cp /usr/local/src/hive/lib/jline-2.12.jar /usr/local/src/hadoop/share/hadoop/yarn/lib/ 初始化后如果想要再重启服务 hive --service metastore & hive --service hiveserver2 & 这个在1.4.7中需要配置，否则在执行数据导入到hive时会报错 Could not load org.apache.hadoop.hive.conf.HiveConf. Make sure HIVE_CONF_DIR is set correctly. cp /usr/local/src/hive/lib/hive-exec-1.1.0.jar /usr/local/src/sqoop/lib/ 三、多用户配置 把单用户配置的hive文件夹复制给node04(hive客户端) scp -r /usr/local/hive node04:/usr/local/src 启动服务端 hive --service metastore #之后会发现阻塞因为客户端正在监听它 查看监听端口，默认为9083,这个在客户端要配置 ss -nal 修改客户端hadoop04中的hive-site.xml文件（把原有的删除） hive.metastore.warehouse.dir /user/hive_remote/warehouse hive.metastore.local false hive.metastore.uris thrift://node02:9083 因为版本原因,需要复制jar包 先在hive启动服务端（此步骤已在上面已做完），然后在hive客户端输入hive访问 hive 常见问题 Could not create ServerSocket on address 0.0.0.0/0.0.0.0:9083. 说明hive服务器端已经启动，需要杀死后，重新启动，或者重启hadoop集群 com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Specified key was too long; max key length is 767 bytes 解决：需要进入mysql目录下删除创建的hive库 四、使用 hive 进入hive数据库 查看数据库，使用数据库 show databases; use default; 查看表 show tables; 创建hive表 \";\" ASCI码为 \"\\073\" ROW FORMAT DELIMITED 每条数据按行拆分 FIELDS TERMINATED BY '\\073' 每行数据字段按 \";\" 拆分 STORED AS TEXTFILE 保存为文本文件 create table film (name string, startTime date, endTime date, company string, director string, actor string, type string, price float, score float) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\073' STORED AS TEXTFILE; 将本地文本导入 hive 先在 shell 中生成文本文件 echo \"电影名称;上映时间;闭映时间;出品公司;导演;主角;影片类型;票房/万;评分; 《熊出没之夺宝熊兵》;2014.1.17;2014.2.23;深圳华强数字动漫有限公司;丁亮;熊大，熊二，; 《菜鸟》;2015.3.27;2015.4.12;麒麟影业公司;项华祥;柯有伦，崔允素，张艾青，刘亚鹏，张星宇;爱情/动作/喜剧;192.0 ;4.5 ; 《栀子花开》;2015.7.10;2015.8.23;世纪百年影业，文投基金，华谊兄弟，千和影业，剧角映画，合一影业等;何炅;李易峰，张慧雯，蒋劲夫，张予曦，魏大勋，李心艾，杜天皓，宋轶，王佑硕，柴格，张云龙;青春，校园，爱情;37900.8 ;4.0 ; 《我是大明星》;2015.12.20;2016.1.31;北京中艺博悦传媒;张艺飞;高天，刘波，谭皓，龙梅子;爱情 励志 喜剧;9.8 ;2.5 ; 《天将雄师》;2015.2.19;2015.4.6;耀莱文化，华谊兄弟，上海电影集团;李仁港;成龙，约翰·库萨克，阿德里安·布劳迪，崔始源 ，林鹏，王若心，筷子兄弟，西蒙子，冯绍峰，朱佳煜;动作，古装，剧情，历史;74430.2 ;5.9 ;\" > /root/film.csv #把数据放到root目录下的film.csv 写到film表中 load data local inpath '/root/film.csv' overwrite into table film; 将hdfs中数据导入hive hdfs dfs -put /root/film.csv /home hdfs dfs -cat /home/film.csv load data inpath '/home/film.csv'into table film; 从 hive 导出到本地文件系统 insert overwrite local directory '/root/hl' select * from film; 或直接在shell中执行 hive -e \"select * from film\" >> film1.csv 或者是以下命令， 其中文件sql.q写入你想要执行的查询语句 hive -f sql.q >> film1.csv 从 hive 导出到 hdfs 中 注意，和导出文件到本地文件系统的HQL少一个local，数据的存放路径就不一样了。 insert overwrite directory '/root/hl' select * from film; hive 中使用dfs命令 hive> dfs -ls /user; 查看表结构信息 desc formatted film; desc film; 查看表信息 select * from film; # 查看前十项 select * from film limit 10; select count(price) from film; # distinct 去重 select count(distinct price) from film; select sum(price) from film; select avg(price) from film; 将提取到的数据保存到临时表中 insert overwrite table movies 本地加载 load data local inpath '/Users/tifa/Desktop/1.txt' into table test; 从hdfs上加载数据 load data inpath '/user/hadoop/1.txt' into table test_external; 抹掉之前的数据重写 load data inpath '/user/hadoop/1.txt' overwrite into table test_external; CREATE TABLE movies( name string, data string, record int ) COMMENT '2014全年上映电影的数据记录' FIELDS TERMINATED BY '\\t' STORED AS TEXTFILE; load data local inpath 'dat0204.log' into table movies; 删除表 DROP TABLE if exists film; "},"MyNotes/大数据组件内容/Hbase相关.html":{"url":"MyNotes/大数据组件内容/Hbase相关.html","title":"Hbase相关","keywords":"","body":"Hbase相关 一、介绍 与传统数据库的对比 传统数据库遇到的问题： 数据量很大的时候无法存储； 没有很好的备份机制； 数据达到一定数量开始缓慢，很大的话基本无法支撑； HBASE优势： 线性扩展，随着数据量增多可以通过节点扩展进行支撑； 数据存储在hdfs上，备份机制健全； 通过zookeeper协调查找数据，访问速度快。 HBase中的表一般有这样的特点： 大：一个表可以有上亿行，上百万列 面向列:面向列(族)的存储和权限控制，列(族)独立检索。 稀疏:对于为空(null)的列，并不占用存储空间，因此，表可以设计的非常稀疏。 二、搭建 配置环境变量 进入配置文件目录 cd /usr/local/src/hbase/conf 编辑hbase-env.sh文件（vim hbase-env.sh） export JAVA_HOME=/usr/local/src/jdk 编辑hbase-site.xml 文件（vim hbase-site.xml） hbase.rootdir hdfs://hp1:9000/hbase hbase.cluster.distributed true hbase.zookeeper.quorum hp1,hp2,hp3 hbase.zookeeper.property.dataDir /usr/local/src/zookeeper/data/ 编辑regionservers文件 （vim regionservers） node02 node03 将hbase所有配置拷贝到其他所有节点 scp -r /usr/local/src/hbase/ node02:/usr/local/src/ scp -r /usr/local/src/hbase/ node03:/usr/local/src/ 启动 hbase 启动 HBase 时需要确保 hdfs 已经启动，使用命令 hdfs dfsadmin -report 查看以下 HDFS 集群是否正常 如果正常，在 master 节点上执行以下命令启动 HBase 集群: start-hbase.sh 测试 hbase shell web 访问 hbase http://node01:16010 三、使用 hbase shell 常用命令 hbase shell 创建表 create 'users','user_id','address','info' 说明:表users,有三个列族user_id,address,info 列出全部表 list 得到表的描述 describe 'users' 创建表 create 'users_tmp','user_id','address','info' 删除表 disable 'users_tmp' drop 'users_tmp' 添加记录 put ‘表名’,’行键(标识)’,’列族:字段’,’数值’ put 'users','yani','info:age','24'; put 'users','yani','info:birthday','1987-06-17'; 获取一条记录 取得一个id的所有数据 get 'users','yani' 获取一个id，一个列族的所有数据 get 'users','yani','info' 获取一个id，一个列族中一个列的所有数据 get 'users','yani','info:age' 更新记录 put 'users','yani','info:age' ,'29' get 'users','yani','info:age' 获取单元格数据的版本数据 get 'users','yani',{COLUMN=>'info:age',VERSIONS=>1} 获取单元格数据的某个版本数据 get 'users','yani',{COLUMN=>'info:age',TIMESTAMP=>1364874937056} 全表扫描 scan 'users' 删除yani值的'info:age'字段 delete 'users','yani','info:age' get 'users','yani' 删除整行 deleteall 'users','yani' 统计表的行数 count 'users' 清空表 truncate 'users' "},"MyNotes/大数据组件内容/Sqoop相关.html":{"url":"MyNotes/大数据组件内容/Sqoop相关.html","title":"Sqoop相关","keywords":"","body":"Sqoop相关 一、介绍 sqoop是hadoop和关系数据库服务器之间传送数据的工具。 sqoop 学习 sqoop 架构 sqoop的常用命令 1、sqoop的主要功能 导入、迁入 导入数据：mysql、oracle 导入数据到 hadoop 的 hdfs、hive、hbase 等数据存储系统 导出、迁出 导出数据：从 hadoop 的文件系统中导出数据到关系数据库 mysql 等 2、sqoop 与 hive sqoop: 工具：本质就是迁移数据，迁移的方式：就是把sqoop的迁移命令转换成MR程序 hive： 工具：本质就是执行计算，依赖于HDFS存储数据，把SQL转换成MR程序 与以下组件可能打交道 HDFS、MapReduce、YARN、ZooKeeper、Hive、HBase、MySQL 二、安装部署 配置环境变量 进入配置文件目录 cd /usr/local/src/sqoop/conf 复制环境变量 sqoop-env.sh 文件 cp sqoop-env-template.sh sqoop-env.sh 修改sqoop-env.sh文件 （vim sqoop-env.sh ） 没用到 Hive 和 HBase 可以不用配置相关项，使用时会弹出警告 export HADOOP_COMMON_HOME=/usr/local/src/hadoop export HADOOP_MAPRED_HOME=/usr/local/src/hadoop export HIVE_HOME=/usr/local/src/hive export ZOOKEEPER_HOME=/usr/local/src/zookeeper export ZOOCFGDIR=/usr/local/src/zookeeper/conf export HBASE_HOME=/usr/local/src/hbase 复制 mysql 数据库驱动包到指定文件路径 cp mysql-connector-java-5.1.46-bin.jar /usr/local/src/sqoop/lib/ 验证 sqoop version 链接测试 sqoop list-databases --connect jdbc:mysql://hp1:3306/ --username root --password qwe 三、常见使用 列出MySQL数据有哪些数据库 sqoop list-databases --connect jdbc:mysql://node01:3306/ --username root --password qwe 列出MySQL中的某个数据库有哪些数据表： sqoop list-tables --connect jdbc:mysql://node01:3306/mysql --username root --password qwe 在hive中创建一张跟mysql中的help_keyword表一样的hive表hk： sqoop create-hive-table \\ --connect jdbc:mysql://node01:3306/mysql \\ --username root \\ --password root \\ --table help_keyword \\ --hive-table hk 四、Sqoop的数据导入，import 从非关系性数据库（mysql、oracle）导入到关系性数据库（hdfs、hbase、hive），条记录可表示为文本、二进制文件或SequenceFile格式 语法格式 sqoop import (generic-args) (import-args) 常用参数 --connect JDBC连接符: jdbc:mysql://node1/movie jdbc:oracle:thin:@//ndoe/movie --connection-manager 连接管理者 --hadoop-mapred-home 指定$HADOOP_MAPRED_HOME路径 conf中指定后无需设置 --dirver JDBC驱动器类 比如com.mysql.jdbc.Driver --username 数据库用户 --password 数据库密码 --password-alias Credential provider password alias --password-file 设置用于存放认证的密码信息文件的路径 --table 导出的表名 --where 配合table使用 --target-dir HDFS目录名 --as-textfile --as-parquetfile --as-avrodatafile --as-sequencefile 保存格式，默认text -m, -num-mappers 启动的Map Task数目 任务并行度， 默认1 -e，--query 取数sql --fields-terminated-by 分割符 --verbose 日志 --append 将数据追加到HDFS上一个已存在的数据集上 -P 从命令行输入密码 --password 密码 --username 账号 --verbose 打印流程信息 --connection-param-file 可选参数 从mysql中导入到HDFS中 普通导入：导入mysql库中的help_keyword的数据到HDFS上 导入的默认路径：/user/hadoop/help_keyword sqoop import \\ --connect jdbc:mysql://hp1:3306/mysql \\ --username root \\ --password qwe \\ --table help_keyword \\ -m 1 查看导入的文件 hdfs dfs -cat /user/root/help_keyword/part-m-00000 导入： 指定分隔符和导入路径 sqoop import \\ --connect jdbc:mysql://node01:3306/mysql \\ --username root \\ --password root \\ --table help_keyword \\ --target-dir /user/hp1/my_help_keyword1 \\ --fields-terminated-by '\\t' \\ -m 2 导入数据：带where条件 sqoop import \\ --connect jdbc:mysql://hp1:3306/mysql \\ --username root \\ --password qwe \\ --where \"name='STRING' \" \\ --table help_keyword \\ --target-dir /sqoop/hp1/myoutport1 \\ -m 1 查询指定列 sqoop import \\ --connect jdbc:mysql://hp1:3306/mysql \\ --username root \\ --password qwe \\ --columns \"name\" \\ --where \"name='STRING' \" \\ --table help_keyword \\ --target-dir /sqoop/hp1/myoutport22 \\ -m 1 selct name from help_keyword where name = \"string\" 导入：指定自定义查询SQL sqoop import \\ --connect jdbc:mysql://hp1:3306/ \\ --username root \\ --password qwe \\ --target-dir /user/hadoop/myimport33_1 \\ --query 'select help_keyword_id,name from mysql.help_keyword where $CONDITIONS and name = \"STRING\"' \\ --split-by help_keyword_id \\ --fields-terminated-by '\\t' \\ -m 4 把MySQL数据库中的表数据导入到Hive中 sqoop 导入关系型数据到 hive 的过程是先导入到 hdfs，然后再 load 进入 hive 普通导入：数据存储在默认的default hive库中，表名就是对应的mysql的表名： sqoop import \\ --connect jdbc:mysql://hp1:3306/mysql \\ --username root \\ --password qwe \\ --table help_keyword \\ --hive-import \\ -m 1 导入过程 第一步：导入mysql.help_keyword的数据到hdfs的默认路径 第二步：自动仿造mysql.help_keyword去创建一张hive表, 创建在默认的default库中 第三步：把临时目录中的数据导入到hive表中 查看数据 hdfs dfs -cat /user/hive/warehouse/help_keyword/part-m-00000 指定行分隔符和列分隔符，指定hive-import，指定覆盖导入，指定自动创建hive表，指定表名，指定删除中间结果数据目录 sqoop import \\ --connect jdbc:mysql://hp1:3306/mysql \\ --username root \\ --password qwe \\ --table help_keyword \\ --fields-terminated-by \"\\t\" \\ --lines-terminated-by \"\\n\" \\ --hive-import \\ --hive-overwrite \\ --create-hive-table \\ --delete-target-dir \\ --hive-database mydb_test \\ --hive-table new_help_keyword 报错原因是hive-import 当前这个导入命令。 sqoop会自动给创建hive的表。 但是不会自动创建不存在的库 手动创建mydb_test数据块 hive> create database mydb_test; OK Time taken: 6.147 seconds hive> 之后再执行上面的语句没有报错 查询一下 select * from new_help_keyword limit 10; 上面的导入语句等价于 sqoop import \\ --connect jdbc:mysql://hp1:3306/mysql \\ --username root \\ --password qwe \\ --table help_keyword \\ --fields-terminated-by \"\\t\" \\ --lines-terminated-by \"\\n\" \\ --hive-import \\ --hive-overwrite \\ --create-hive-table \\ --hive-table mydb_test.new_help_keyword \\ --delete-target-dir 增量导入 --incremental append --check-column 检查的字段 --last-value 起始字段last-value + 1 执行增量导入之前，先清空hive数据库中的help_keyword表中的数据 truncate table help_keyword; sqoop import \\ --connect jdbc:mysql://hp1:3306/mysql \\ --username root \\ --password qwe \\ --table help_keyword \\ --target-dir /user/hadoop/myimport_add \\ --incremental append \\ --check-column help_keyword_id \\ --last-value 500 \\ -m 1 把MySQL数据库中的表数据导入到hbase sqoop import \\ --connect jdbc:mysql://hp1:3306/mysql \\ --username root \\ --password qwe \\ --table help_keyword \\ --hbase-table new_help_keyword \\ --column-family person \\ --hbase-row-key help_keyword_id 五、Sqoop的数据导出，export 从Hadoop(HDFS、HBase、Hive)导出到关系型数据库(Mysql、Oracle) export连接配置参数同import sqoop export --connect jdbc:mysql://hp1:3306/mysql \\ --table data --export-dir /user/x/data/ \\ --username root \\ --password qwe \\ --update-key id \\ --update-mode allowinsert 从 hdfs 导出到 mysql sqoop export \\ --connect jdbc:mysql://hadoop01:3306/test \\ --username hadoop \\ --password qwe \\ --table book \\ --export-dir /sqoopdata \\ --fields-terminated-by ',' 从 hive 导出到 mysql sqoop export \\ --connect jdbc:mysql://hadoop01:3306/test \\ --username hadoop \\ --password qwe \\ --table book \\ --export-dir /user/hive/warehouse/uv/dt=2011-08-03 \\ --input-fileds-terminated-by '\\t' 从 hbase 导出到 mysql 默认的没有命令直接将hbase中的数据导入到MySQL，因为在hbase中的表数据量通常比较大，如果一次性导入到MySQL，可能导致MySQL直接崩溃。 但是可以用别的办法进行导入： 将 Hbase 数据，扁平化成 HDFS 文件，然后再由 sqoop 导入 将 Hbase 数据导入 Hive 表中，然后再导入 mysql 直接使用 Hbase 的 Java API 读取表数据，直接向 mysql 导入，不需要使用 sqoop "},"MyNotes/大数据组件内容/Flume相关.html":{"url":"MyNotes/大数据组件内容/Flume相关.html","title":"Flume相关","keywords":"","body":"Flume相关 一.介绍 Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的软件。Flume基于流式架构，灵活简单。 Flume的核心是把数据从数据源(source)收集过来，再将收集到的数据送到指定的目的地(sink)。为了保证输送的过程一定成功，在送到目的地(sink)之前，会先缓存数据(channel),待数据真正到达目的地(sink)后，flume再删除自己缓存的数据。 ) Flume的优点 1.可以高速采集数据，采集的数据能够以想要的文件格式及压缩方式存储在hdfs上 2.事务功能保证了数据在采集的过程中数据不丢失 3.部分Source保证了Flume挂了以后重启依旧能够继续在上一次采集点采集数据，真正做到数据零丢失 Flume的组成架构 Agent Agent是一个JVM进程，它以事件的形式将数据从源头送至目的 Agent主要有3个部分组成：Source、Channel、Sink Source Source是负责接收数据的Flume Agent的组件。Source组件可以处理各种类型、各种格式的日志数据，包括：avro、thrift、exec、jms、spooling directory(spooldir文件夹1.7之后出现)、netcat、sequence generator、syslog、http、legacy。 Channel Channel是位于Source和Sink之间的缓冲区。因此，Channel允许Source和Sink运作在不同的速率上。Channel是线程安全的，可以处理几个Source的写入操作和几个Sink的读取操作 Flume自带两种Channel： Memory Channel（内存中的队列） File Channel（将所有事件写到磁盘） Sink Sink不断地轮询Channel中的是事件且批量地移除它们，并将这些事件批量写入到储存或索引系统、或者被发送到另一个Flume Agent。 Sink组件目的地包括：hdfs、logger、avro、thrift、ipc、file、null、Hbase、solr、自定义。 Flume Agent的内部原理 ) 二.搭建 Flume镜像网:http://mirror.bit.edu.cn/apache/flume/ 配置Flume环境 wget http://mirror.bit.edu.cn/apache/flume/1.6.0/apache-flume-1.6.0-bin.tar.gz tar -zxvf apache-flume-1.6.0-bin.tar.gz -C /usr/local/src mv apache-flume-1.6.0-bin.tar.gz flume 修改flume-env.sh cd /usr/local/src/flume/conf cp -p flume-env.sh.template flume-env.sh vim flume-env.sh #修改为： JAVA_HOME=/usr/local/src/java 启动（具体看案例） 三.案例 监控端口数据官方案例 案例需求 服务端：首先启动Flume任务，监控本机55555端口 客户端：然后通过netcat工具向本机55555端口发送消息 最后Flume将监听的数据实时显示在控制台。 实现步骤 安装netcat工具 yum -y install nc 创建Flume Agent配置文件flume-netcat-logger.conf #在flume配置文件根目录创建Agent配置文件夹 mkdir job cd job vim flume-netcat-logger.conf ​ 添加以下内容： # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = netcat a1.sources.r1.bind = 0.0.0.0 a1.sources.r1.port = 55555 # Describe/configure the sink a1.sinks.k1.type = logger # Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to rhe channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 启动 #在flume配置文件根目录下 #第一种写法： bin/flume-ng agent --conf conf/ --name a1 --conf-file job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console #第二种写法： bin/flume-ng agent -c conf/ -n a1 -f job/flume-netcat-logger.conf -Dflume.root.logger=INFO,console 实时读取本地文件到HDFS案例 案例需求 实时监控apache日志，并上传到HDFS中 实现步骤 创建Flume Agent配置文件flume-file-hdfs.conf #在flume配置文件根目录创建Agent配置文件夹 cd job vim flume-file-hdfs.conf ​ 添加以下内容： # Name the components on this agent a2.sources = r2 a2.sinks = k2 a2.channels = c2 # Describe/configure the source a2.sources.r2.type = exec a2.sources.r2.command = tail -F /etc/httpd/logs/access_log a2.sources.r2.shell = /bin/bash -c # Describe/configure the sink a2.sinks.k2.type = hdfs a2.sinks.k2.hdfs.path = hdfs://node01:9000/flume/%Y%m%d/%H #上传文件的前缀 a2.sinks.k2.hdfs.filePrefix = logs- #是否按照时间滚动文件夹 a2.sinks.k2.hdfs.round = true #多少时间单位创建一个新的文件夹 a2.sinks.k2.hdfs.roundValue = 1 #重新定义时间单位 a2.sinks.k2.hdfs.roundUnit = hour #是否使用本地时间戳 a2.sinks.k2.hdfs.useLocalTimeStamp = true #积攒多少个Event才flush到HDFS一次 a2.sinks.k2.hdfs.batchSize = 1000 #设置文件类型，可支持压缩 a2.sinks.k2.hdfs.fileType = DataStream #多久生成一个新的文件 a2.sinks.k2.hdfs.rollInterval = 60 #设置每一个文件的滚动大小 a2.sinks.k2.hdfs.rollSize = 134217700 #文件的滚动与Event数量无关 a2.sinks.k2.hdfs.rollCount = 0 # Use a channel which buffers events in memory a2.channels.c2.type = memory a2.channels.c2.capacity = 1000 a2.channels.c2.transactionCapacity = 100 # Bind the source and sink to rhe channel a2.sources.r2.channels = c2 a2.sinks.k2.channel = c2 启动 #在flume配置文件根目录下 bin/flume-ng agent -c conf/ -n a2 -f job/flume-file-hdfs.conf 实时读取本地文件夹文件到HDFS案例 实现步骤 创建Flume Agent配置文件flume-dir-hdfs.conf #在flume配置文件根目录创建Agent配置文件夹 cd job vim flume-dir-hdfs.conf ​ 添加以下内容： # Name the components on this agent a3.sources = r3 a3.sinks = k3 a3.channels = c3 # Describe/configure the source a3.sources.r3.type = spooldir a3.sources.r3.spoolDir = /usr/local/test a3.sources.r3.fileSuffix = .COMPLETED a3.sources.r3.fileHeader = true #忽略所以以.tmp结尾的文件，不上传 a3.sources.r3.ignorePattern = ([^ ]*\\.tmp) # Describe/configure the sink a3.sinks.k3.type = hdfs a3.sinks.k3.hdfs.path = hdfs://node01:9000/flume/upload/%Y%m%d/%H #上传文件的前缀 a3.sinks.k3.hdfs.filePrefix = upload- #是否按照时间滚动文件夹 a3.sinks.k3.hdfs.round = true #多少时间单位创建一个新的文件夹 a3.sinks.k3.hdfs.roundValue = 1 #重新定义时间单位 a3.sinks.k3.hdfs.roundUnit = hour #是否使用本地时间戳 a3.sinks.k3.hdfs.useLocalTimeStamp = true #积攒多少个Event才flush到HDFS一次 a3.sinks.k3.hdfs.batchSize = 1000 #设置文件类型，可支持压缩 a3.sinks.k3.hdfs.fileType = DataStream #多久生成一个新的文件 a3.sinks.k3.hdfs.rollInterval = 60 #设置每一个文件的滚动大小 a3.sinks.k3.hdfs.rollSize = 134217700 #文件的滚动与Event数量无关 a3.sinks.k3.hdfs.rollCount = 0 # Use a channel which buffers events in memory a3.channels.c3.type = memory a3.channels.c3.capacity = 1000 a3.channels.c3.transactionCapacity = 100 # Bind the source and sink to rhe channel a3.sources.r3.channels = c3 a3.sinks.k3.channel = c3 启动 #在flume配置文件根目录下 bin/flume-ng agent -c conf/ -n a2 -f job/flume-dir-hdfs.conf 单数据源多出口案例(选择器) 案例需求 使用Flume-1监控文件变动，Flume-1将变动内容传递给Flume-2，Flume-2负责储存到HDFS。同时Flume-1将变动内容传递给Flume-3，Flume-3负责输出到Local FileSystem（2个sink，2个channel） 实现步骤 创建Flume Agent配置文件flume-file-avro.conf mkdir job/project1 cd job/project1 vim flume-file-avro.conf ​ 添加以下内容： # Name the components on this agent flume1.sources = r1 flume1.sinks = k1 k2 flume1.channels = c1 c2 #将数据流复制给所有channel flume1.sources.r1.selector.type = replicating # Describe/configure the source flume1.sources.r1.type = exec flume1.sources.r1.command = tail -F /etc/httpd/logs flume1.sources.r1.shell = /bin/bash -c # Describe/configure the sink flume1.sinks.k1.type = avro flume1.sinks.k1.hostname = node01 flume1.sinks.k1.port = 4210 flume1.sinks.k2.type = avro flume1.sinks.k2.hostname = node01 flume1.sinks.k2.port = 4211 # Use a channel which buffers events in memory flume1.channels.c1.type = memory flume1.channels.c1.capacity = 1000 flume1.channels.c1.transactionCapacity = 100 flume1.channels.c2.type = memory flume1.channels.c2.capacity = 1000 flume1.channels.c2.transactionCapacity = 100 # Bind the source and sink to rhe channel flume1.sources.r1.channels = c1 c2 flume1.sinks.k1.channel = c1 flume1.sinks.k2.channel = c2 创建Flume Agent配置文件flume-avro-local.conf cd job/project1 vim flume-avro-local.conf ​ 添加以下内容： # Name the components on this agent flume2.sources = r2 flume2.sinks = k2 flume2.channels = c2 # Describe/configure the source flume2.sources.r2.type = avro flume2.sources.r2.bind = node01 flume2.sources.r2.port = 4210 # Describe/configure the sink flume2.sinks.k2.type = file_roll flume2.sinks.k2.directory = /usr/local/flume2 # Use a channel which buffers events in memory flume2.channels.c2.type = memory flume2.channels.c2.capacity = 1000 flume2.channels.c2.transactionCapacity = 100 # Bind the source and sink to rhe channel flume2.sources.r2.channels = c2 flume2.sinks.k2.channel = c2 提示：输出的本地目录必须是已经存在的目录，如果该目录不存在，并不会创建新的目录 创建Flume Agent配置文件flume-avro-hdfs.conf cd job/project1 vim flume-avro-hdfs.conf ​ 添加以下内容： # Name the components on this agent flume3.sources = r3 flume3.sinks = k3 flume3.channels = c3 # Describe/configure the source flume3.sources.r3.type = avro flume3.sources.r3.bind = node01 flume3.sources.r3.port = 4211 # Describe/configure the sink flume3.sinks.k3.type = hdfs flume3.sinks.k3.hdfs.path = hdfs://node01:9000/flume/project1/%Y%m%d/%H #上传文件的前缀 a3.sinks.k3.hdfs.filePrefix = upload- #是否按照时间滚动文件夹 a3.sinks.k3.hdfs.round = true #多少时间单位创建一个新的文件夹 a3.sinks.k3.hdfs.roundValue = 1 #重新定义时间单位 a3.sinks.k3.hdfs.roundUnit = hour #是否使用本地时间戳 a3.sinks.k3.hdfs.useLocalTimeStamp = true #积攒多少个Event才flush到HDFS一次 a3.sinks.k3.hdfs.batchSize = 1000 #设置文件类型，可支持压缩 a3.sinks.k3.hdfs.fileType = DataStream #多久生成一个新的文件 a3.sinks.k3.hdfs.rollInterval = 60 #设置每一个文件的滚动大小 a3.sinks.k3.hdfs.rollSize = 134217700 #文件的滚动与Event数量无关 a3.sinks.k3.hdfs.rollCount = 0 # Use a channel which buffers events in memory flume3.channels.c3.type = memory flume3.channels.c3.capacity = 1000 flume3.channels.c3.transactionCapacity = 100 # Bind the source and sink to rhe channel flume3.sources.r3.channels = c3 flume3.sinks.k3.channel = c3 启动 #先启动flume2或者flume3，最后再启动flume1 bin/flume-ng agent -c conf/ -n flume2 -f job/project1/flume-avro-local.conf bin/flume-ng agent -c conf/ -n flume3 -f job/project1/flume-avro-hdfs.conf bin/flume-ng agent -c conf/ -n flume1 -f job/project1/flume-file-avro.conf 单数据源多出口案例(sinks组) 案例需求 使用Flume-1监控文件变动，Flume-1将变动内容传递给Flume-2，Flume-2负责储存到控制台。同时Flume-1将变动内容传递给Flume-3，Flume-3负责输出到控制台（2个sink，1个channel,1个sinkgroup组,轮循原理） 实现步骤 创建Flume Agent配置文件flume-netcat-avro.conf mkdir job/project2 cd job/project2 vim flume-netcat-avro.conf ​ 添加以下内容： # Name the components on this agent flume1.sources = r1 flume1.sinks = k1 k2 flume1.channels = c1 flume1.sinkgroups = g1 #configure the sinkgroups #设置类型为负载均衡 flume1.sinkgroups.g1.processor.type = load_balance #设置是否回滚 flume1.sinkgroups.g1.processor.backoff = true #设置为轮循 flume1.sinkgroups.g1.processor.selector = round_robin #设置超时的时间 flume1.sinkgroups.g1.processor.selector.maxTimeOut = 10000 # Describe/configure the source flume1.sources.r1.type = netcat flume1.sources.r1.bind = 0.0.0.0 flume1.sources.r1.port = 4455 # Describe/configure the sink flume1.sinks.k1.type = avro flume1.sinks.k1.hostname = node01 flume1.sinks.k1.port = 4210 flume1.sinks.k2.type = avro flume1.sinks.k2.hostname = node01 flume1.sinks.k2.port = 4211 # Use a channel which buffers events in memory flume1.channels.c1.type = memory flume1.channels.c1.capacity = 1000 flume1.channels.c1.transactionCapacity = 100 # Bind the source and sink to rhe channel flume1.sources.r1.channels = c1 flume1.sinkgroups.g1.sinks = k1 k2 flume1.sinks.k1.channel = c1 flume1.sinks.k2.channel = c1 创建Flume Agent配置文件flume-avro-console1.conf cd job/project2 vim flume-avro-console1.conf ​ 添加以下内容： # Name the components on this agent flume2.sources = r2 flume2.sinks = k2 flume2.channels = c2 # Describe/configure the source flume2.sources.r2.type = avro flume2.sources.r2.bind = node01 flume2.sources.r2.port = 4210 # Describe/configure the sink flume2.sinks.k2.type = logger # Use a channel which buffers events in memory flume2.channels.c2.type = memory flume2.channels.c2.capacity = 1000 flume2.channels.c2.transactionCapacity = 100 # Bind the source and sink to rhe channel flume2.sources.r2.channels = c2 flume2.sinks.k2.channel = c2 创建Flume Agent配置文件flume-avro-console2.conf cd job/project2 vim flume-avro-console2.conf ​ 添加以下内容： # Name the components on this agent flume3.sources = r3 flume3.sinks = k3 flume3.channels = c3 # Describe/configure the source flume3.sources.r3.type = avro flume3.sources.r3.bind = node01 flume3.sources.r3.port = 4211 # Describe/configure the sink flume3.sinks.k3.type = logger # Use a channel which buffers events in memory flume3.channels.c3.type = memory flume3.channels.c3.capacity = 1000 flume3.channels.c3.transactionCapacity = 100 # Bind the source and sink to rhe channel flume3.sources.r3.channels = c3 flume3.sinks.k3.channel = c3 启动 #先启动flume2或者flume3，最后再启动flume1 bin/flume-ng agent -c conf/ -n flume2 -f job/project2/flume-avro-console1.conf -Dflume.root.logger=INFO,console bin/flume-ng agent -c conf/ -n flume3 -f job/project2/flume-avro-console2.conf -Dflume.root.logger=INFO,console bin/flume-ng agent -c conf/ -n flume1 -f job/project2/flume-netcat-avro.conf 多数据源汇总案例 案例需求 node01上的Flume-1监控文件/etc/httpd/logs/access_log node02上的Flume-2监控某一个端口的数据流 Flume-1和Flume-2将数据发送给node03上的Flume-3 Flume-3最终数据打印到控制台 实现步骤 在node01上创建Flume Agent配置文件flume-exec-avro.conf mkdir /usr/local/src/flume/job/project3 cd /usr/local/src/flume/job/project3 vim flume-exec-avro.conf ​ 添加以下内容： # Name the components on this agent flume1.sources = r1 flume1.sinks = k1 flume1.channels = c1 # Describe/configure the source flume1.sources.r1.type = exec flume1.sources.r1.command = tail -F /etc/httpd/logs/access_log flume1.sources.r1.shell = /bin/bash -c # Describe/configure the sink flume1.sinks.k1.type = avro flume1.sinks.k1.hostname = node03 flume1.sinks.k1.port = 4210 # Use a channel which buffers events in memory flume1.channels.c1.type = memory flume1.channels.c1.capacity = 1000 flume1.channels.c1.transactionCapacity = 100 # Bind the source and sink to rhe channel flume1.sources.r1.channels = c1 flume1.sinks.k1.channel = c1 在node02上创建Flume Agent配置文件flume-netcat-avro.conf mkdir /usr/local/src/flume/job/project3 cd /usr/local/src/flume/job/project3 vim flume-netcat-avro.conf ​ 添加以下内容： # Name the components on this agent flume1.sources = r1 flume1.sinks = k1 flume1.channels = c1 # Describe/configure the source flume1.sources.r1.type = netcat flume1.sources.r1.bind = 0.0.0.0 flume1.sources.r1.port = 5656 # Describe/configure the sink flume1.sinks.k1.type = avro flume1.sinks.k1.hostname = node03 flume1.sinks.k1.port = 4210 # Use a channel which buffers events in memory flume1.channels.c1.type = memory flume1.channels.c1.capacity = 1000 flume1.channels.c1.transactionCapacity = 100 # Bind the source and sink to rhe channel flume1.sources.r1.channels = c1 flume1.sinks.k1.channel = c1 在node03上创建Flume Agent配置文件flume-avro-console.conf mkdir /usr/local/src/flume/job/project3 cd /usr/local/src/flume/job/project3 vim flume-avro-console.conf ​ 添加以下内容： # Name the components on this agent flume1.sources = r1 flume1.sinks = k1 flume1.channels = c1 # Describe/configure the source flume1.sources.r1.type = avro flume1.sources.r1.bind = node03 flume1.sources.r1.port = 4210 # Describe/configure the sink flume1.sinks.k1.type = logger # Use a channel which buffers events in memory flume1.channels.c1.type = memory flume1.channels.c1.capacity = 1000 flume1.channels.c1.transactionCapacity = 100 # Bind the source and sink to rhe channel flume1.sources.r1.channels = c1 flume1.sinks.k1.channel = c1 启动 #先启动node03的flume bin/flume-ng agent -c conf/ -n flume1 -f job/project3/flume-avro-console.conf -Dflume.root.logger=INFO,console #再启动node01的flume bin/flume-ng agent -c conf/ -n flume1 -f job/project3/flume-exec-avro.conf #最后启动node02的flume bin/flume-ng agent -c conf/ -n flume1 -f job/project3/flume-netcat-avro.conf 四.Flume监控 Ganglia的安装与部署 安装httpd服务和php yum -y install httpd php 安装其他依赖 yum -y install rrdtool perl-rrdtool rrdtool-devel 安装ganglia #更新源 yum -y install epel-release #安装Ganglia的组件（gmetad、web、gmod） yum -y install ganglia-gmetad ganglia-web ganglia-gmond 修改配置文件/etc/httpd/conf.d/ganglia.conf vim /etc/httpd/conf.d/ganglia.conf #配置ganglia前端访问的权限 Order deny,allow Allow from all # Require ip 10.1.2.3 # Require host example.org 修改配置文件 /etc/ganglia/gmetad.conf vim /etc/ganglia/gmetad.conf #修改为： data_source \"node01\" 192.168.1.200 修改配置文件 /etc/ganglia/gmond.conf vim /etc/ganglia/gmond.conf #修改为(和gmetad文件一一对应)： cluster { name = \"node01\" owner = \"unspecified\" latlong = \"unspecified\" url = \"unspecified\" } udp_send_channel { #bind_hostname = yes # Highly recommended, soon to be default. # This option tells gmond to use a source address # that resolves to the machine's hostname. Without # this, the metrics may appear to come from any # interface and the DNS names associated with # those IPs will be used to create the RRDs. # mcast_join = 239.2.11.71 host = 192.168.1.200 port = 8649 ttl = 1 } udp_recv_channel { # mcast_join = 239.2.11.71 port = 8649 bind = 192.168.1.200 retry_bind = true # Size of the UDP buffer. If you are handling lots of metrics you really # should bump it up to e.g. 10MB or even higher. # buffer = 10485760 } 关闭selinux服务 #临时关闭 setenforce 0 启动Ganglia service httpd start service gmetad start service gmond start 打开网页浏览ganglia页面 http://192.168.1.200/ganglia 提示：如果以上操作出现权限不足，请修改/var/lib/ganglia目录的权限： chmod 777 /var/lib/ganglia 操作Flume测试监控 修改flume配置文件中的flume-env.sh export JAVA_OPTS=\"-Dflume.monitoring.type=ganglia -Dflume.monitoring.hosts=192.168.1.200:8649 -Xms100m -Xmx200m\" 启动Flume任务 "},"MyNotes/大数据组件内容/Kafka相关.html":{"url":"MyNotes/大数据组件内容/Kafka相关.html","title":"Kafka相关","keywords":"","body":"Kafka相关 一.介绍 Kafka是一个分布式的基于发布/订阅模式的消息队列，主要应用于大数据实时处理领域 消息队列 消息队列的两种模式 点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除） 消息生产者生产消息发送到Queue中,然后消息消费者从Queue中取出并且消费消息。 消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。 发布/订阅模式（一对多，消费者消费数据之后不会清除消息） 消息生产者（发布）将消息发布到topic中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到topic的消息会被所有订阅者消费。 Kafka基础架构 Producer：消息生产者，就是向Kakfa broker发消息的客户端 Consumer：消息消费者，向kafka取消息的客户端 Consumer Group（CG）：消费者组，由多个consumer组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，既消费者组是逻辑上的一个订阅者。 Broker：一台Kafka服务器就是一个broker。一个集群由多个broker组成，一个broker可以容纳多个topic。 Topic：可以理解为一个队列，生产者和消费者面向的都是一个topic。 Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（服务器）上，一个topic可以分为多个partition，每一个partition是一个有序的队列。 Replica：副本，为保证集群中的某个节点发生故障，该节点上的patition数据不丢失，且kafka仍然能够继续工作，kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个leader和若干个follower。 安装部署 ​ Kafka镜像网址：http://mirror.bit.edu.cn/apache/kafka/ Kafa端口号：9092 集群规划 | node01 | node02 | node03 | | :----: | :----: | :----: | | zk | zk | zk | | kafka | kafka | kafka | 搭建Zookeeper 解压安装包 tar -zxvf kafka_2.12-1.0.2.tgz -C /usr/local/src cd /usr/local/src/ mv kafka_2.12-1.0.2 kafka mkdir kafka/logs 配置环境变量 vim /etc/profile #添加以下内容 export KAFKA_HOME=/usr/local/src/kafka export PATH=$PATH:$KAFKA_HOME/bin 修改配置文件server.properties cd /usr/local/src/kafka/config vim server.properties #修改以下内容： log.dirs=/usr/local/src/kafka/logs delete.topic.enable=true zookeeper.connect=node01:2181,node02:2181,node03:2181 分发给其他节点 scp -r /usr/local/src/kafka node02:/usr/local/src/ scp -r /usr/local/src/kafka node03:/usr/local/src/ #修改每一个节点server.properties配置文件中的broker.id（每一个节点的id不能一样） 启动三个节点的Kafka #启动kafka之前一定先要启动Zookeeper kafka-server-start.sh -daemon /usr/local/src/kafka/config/server.properties Kafka命令行测试 #创建一个topic kafka-topics.sh --zookeeper node02:2181 --create --topic first --partitions 3 --replication-factor 2 #查看topic kafka-topics.sh --zookeeper node02:2181 --list #查看topic中的描述信息 kafka-topics.sh --zookeeper node03:2181 --describe --topic first #发送消息到节点 kafka-console-producer.sh --topic first --broker-list node02:9092,node03:9092 #消费节点的消息 kafka-console-consumer.sh --topic first --bootstrap-server node01:9092,node02:9092 #修改分区，partitions数量只能增加不能减少 kafka-topics.sh --zookeeper node02:2181 --topic first --alter --partitions 4 Kafka架构深入 Kafka工作流程及文件储存机制 Kafka工作流程 Kafka中消息是以topic进行分类的，生产者生产消息，消费者消费消息，都是面向topic的。 topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，且每条数据都有自己的offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个offset，以便出错恢复时，从上次的位置继续消费。 https://blog.csdn.net/qq_35641192/article/details/80956244 https://www.jianshu.com/p/f0687411f631 Kafka文件储存机制 由于生产者生产的消息会不断追加到log文件末尾，为防止log文件过大导致数据定位效率低下，Kaka 采取了分片和索引机制，将每个partition分为多个segment。每个segment对应两个文件-- -- “ .index\"文件和“.log”文件。这些文件位于一个文件夹下，该文件夹的命名规则为: topic 名称+分区序号。例如，first 这个topic有三个分区，则其对应的文件夹为first-0,first-1 ,first-2。 0000000.index 0000000.log 0000005.index 0000005.log 00000010.index 00000010.log ​ index和log文件以当前segment的第一条消息的offset命名。下图为index文件和log文件的结构示意图。 ​ “.index“文件储存大量的索引信息，”.log“文件储存大量的数据，索引文件中的元数据指向对应数据文件中message的物理偏移地址。 https://www.jianshu.com/p/73d49083c896 Kafka生产者 分区策略 分区的原因 方便在集群中扩展。每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了； 可以提高并发。因为可以以Partition为单位读写了。 分区的原则 我们需要将producer发送的数据封装成一个ProducerRecord对象。 指明partition的情况下，直接将指明的值作为partition值； 没有指明partition值但有key的情况下，将key的hash值与topic的partition数进行取余得到partition值 既没有partition值又没有key值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增）,将这个值与topic可以用的partition总数取余得到partition值，也就是常说的round-robin(轮循)算法 数据可靠性保证 为保证producer发送的数据，能可靠的发送到指定的topic, topic的每个patition收到produce发送的数据后,都需要向producer发送ack（acknowledgement确认收到），如果producer收到ack，就会进行下一轮的发送，否则重新发送数据。 https://baijiahao.baidu.com/s?id=1658221660132943620&wfr=spider&for=pc "},"MyNotes/Git相关.html":{"url":"MyNotes/Git相关.html","title":"Git相关","keywords":"","body":"Git相关 一.第一次上传本地项目到GitHUb 1.下载Git软件 Git官网：https://git-scm.com/ 2.上传 #进入到指定位置或者在指定位置打开Git Bash mkdir note cd note git init git add . git commit -m \"note\" #会报错，提示输入账户和名字 第一次上传的时候需要配置GitHub的账号和密码 git config --global user.email \"you@example.com\" git config --global user.name \"Your Name\" git commit -m \"note\" git remote add origin https://github.com/xxxx/xxx.git git push -u origin master #创建GitHub仓库时会有一个链接 二.更新 #更新完文件之后，执行以下命令 git add . git commit -m \"commit\" git push "}}